use Collection;

class App {
	function : Main(args : String[]) ~ Nil {
		App->New()->TrainAndPredict();		
	}

	New() {
	}

	method : public : TrainAndPredict() ~ Nil {
		data := Vector->New()<IntArrayRef>;
		data->AddBack(IntArrayRef->New([115, 66]));
		data->AddBack(IntArrayRef->New([175, 78]));
		data->AddBack(IntArrayRef->New([205, 72]));
		data->AddBack(IntArrayRef->New([120, 67]));

		answers := Vector->New()<FloatRef>;
		answers->AddBack(1.0); answers->AddBack(0.0);
		answers->AddBack(0.0); answers->AddBack(1.0);

		network500 := Network->New(500);
		network500->Train(data, answers);
		
		network1000 := Network->New(1000);
		network1000->Train(data, answers);
		
#~
		System.out.println("");
		System.out.println(String.format("	male, 167, 73: network500: %.10f | network1000: %.10f", network500.predict(167, 73), network1000.predict(167, 73)));
		System.out.println(String.format("female, 105, 67: network500: %.10f | network1000: %.10f", network500.predict(105, 67), network1000.predict(105, 67))); 
		System.out.println(String.format("female, 120, 72: network500: %.10f | network1000: %.10f", network500.predict(120, 72), network1000.predict(120, 72))); 
		System.out.println(String.format("	male, 143, 67: network500: %.10f | network1000: %.10f", network500.predict(143, 67), network1000.predict(120, 72)));
		System.out.println(String.format(" male', 130, 66: network500: %.10f | network1000: %.10f", network500.predict(130, 66), network1000.predict(130, 66)));

		/*
		Network network500learn1 := new Network(500, 2.0);
		network500learn1.train(data, answers);

		Network network1000learn1 := new Network(1000, 2.0);
		network1000learn1.train(data, answers);

		System.out.println("");
		System.out.println(String.format("	male, 167, 73: network500learn1: %.10f | network1000learn1: %.10f", network500learn1.predict(167, 73), network1000learn1.predict(167, 73)));
		System.out.println(String.format("female, 105, 67: network500learn1: %.10f | network1000learn1: %.10f", network500learn1.predict(105, 67), network1000learn1.predict(105, 67))); 
		System.out.println(String.format("female, 120, 72: network500learn1: %.10f | network1000learn1: %.10f", network500learn1.predict(120, 72), network1000learn1.predict(120, 72))); 
		System.out.println(String.format("	male, 143, 67: network500learn1: %.10f | network1000learn1: %.10f", network500learn1.predict(143, 67), network1000learn1.predict(120, 72)));
		System.out.println(String.format(" male', 130, 66: network500learn1: %.10f | network1000learn1: %.10f", network500learn1.predict(130, 66), network1000learn1.predict(130, 66)));
		*/
~#
		"Fin."->PrintLine();
	}
}

class Network {
	@epochs : Int; # 1000;
	@learnFactor : FloatRef;
	@neurons : Vector<Neuron>;
	
	New(epochs : Int) {
		@neurons := Vector->New()<Neuron>;
		each(i : 6) {
			@neurons->AddBack(Neuron->New());
		}

		@epochs := epochs;
	}

	New(epochs : Int, learnFactor : FloatRef) {
		@neurons := Vector->New()<Neuron>;
		each(i : 6) {
			@neurons->AddBack(Neuron->New());
		}

		@epochs := epochs;
		@learnFactor := learnFactor;
	}


	method : public : Predict(input1 : Int, input2 : Int) ~ FloatRef {
		return @neurons->Get(5)->Compute(
			@neurons->Get(4)->Compute(
				@neurons->Get(2)->Compute(input1, input2),
				@neurons->Get(1)->Compute(input1, input2)
			),
			@neurons->Get(3)->Compute(
				@neurons->Get(1)->Compute(input1, input2),
				@neurons->Get(0)->Compute(input1, input2)
			)
		);
	}

	
	method : public : Train(data : Vector<IntArrayRef>, answers : Vector<FloatRef>) ~ Nil {
		bestEpochLoss : FloatRef;

		each(epoch : @epochs) {
			# adapt neuron
			epochNeuron := @neurons->Get(epoch % 6);
			epochNeuron->Mutate(@learnFactor);

			predictions := Vector->New()<FloatRef>;
			each(i : data) {
				array := data->Get(i)->Get();
				predictions->AddBack(Predict(array[0], array[1]));
			};
			thisEpochLoss := Util->MeanSquareLoss(answers, predictions);

			if(epoch % 10 = 0) {
				# System.out.println(String.format("Epoch: %s | bestEpochLoss: %.15f | thisEpochLoss: %.15f", epoch, bestEpochLoss, thisEpochLoss));
			};

			if(bestEpochLoss = Nil){
				bestEpochLoss := thisEpochLoss;
				epochNeuron->Remember();
			}
			else {
				if(thisEpochLoss < bestEpochLoss->Get()) {
					bestEpochLoss := thisEpochLoss;
					epochNeuron->Remember();
				} 
				else {
					epochNeuron->Forget();
				};
			};
		};
	}
}


class Neuron {
	@oldBias, @bias : Float;
	@oldWeight1, @weight1 : Float;
	@oldWeight2, @weight2 : Float;

	New() {
		@oldBias := Float->Random(-1, 1);
		@bias := Float->Random(-1, 1); 
		@oldWeight1 := Float->Random(-1, 1);
		@weight1 := Float->Random(-1, 1); 
		@oldWeight2 := Float->Random(-1, 1);
		@weight2 := Float->Random(-1, 1);
	}

	method : public : Mutate(learnFactor : FloatRef) ~ Nil {
      propertyToChange := Int->Random(0, 3);
      changeFactor := (learnFactor = Nil) ? Float->Random(-1.0, 1.0) : (learnFactor * Float->Random(-1.0, 1.0));      
		if(propertyToChange = 0){ 
			@bias += changeFactor; 
		} 
		else if (propertyToChange = 1){ 
			@weight1 += changeFactor; 
		} 
		else { 
			@weight2 += changeFactor; 
		};
	}

	method : public : Forget() ~ Nil {
		@bias := @oldBias;
		@weight1 := @oldWeight1;
		@weight2 := @oldWeight2;
	}

  method : public : Remember() ~ Nil {
    @oldBias := @bias;
    @oldWeight1 := @weight1;
    @oldWeight2 := @weight2;
  }
  
  method : public : Compute(input1 : Float, input2 : Float) ~ Float {
    preActivation := (@weight1 * input1) + (@weight2 * input2) + @bias;
    output := Util->Sigmoid(preActivation);
    return output;
  }
}

class Util {
  function : Sigmoid(in : Float) ~ Float {
  	return 1 / (1 + Float->Exp(-in));
  }

  function : SigmoidDeriv(in : Float) ~ Float {
  	sigmoid := Sigmoid(in);
  	return sigmoid * (1 - in);
  }

  # Assumes array args are same length
  function : MeanSquareLoss(correctAnswers : Vector<FloatRef>, predictedAnswers : Vector<FloatRef>) ~ Float {
    sumSquare := 0;
    each(i : correctAnswers) {
      error := correctAnswers->Get(i) - predictedAnswers->Get(i);
      sumSquare += (error * error);
    };

    return sumSquare / (correctAnswers->Size());
  }
}