use System.Matrix;

class NeuralNetwork {
	@input_nodes : Float;
	@hidden_nodes : Float;
	@output_nodes : Float;
	@learning_rate : Float;
	@weight_inputs_hidden : Float[,];
	@weight_outputs_hidden : Float[,];

	function : Main(args : String[]) ~ Nil {
		if(args->Size() = 2) {
			Brun(args[0]->Equals("true"), args[1]->Equals("true"));
		}
	}

	function : native : Brun(left : Bool, right : Bool) ~ Nil {
		input_nodes := 2;
		hidden_nodes := 2;
		output_nodes := 1;
		learning_rate := 0.15;
		iterations := 5000;

		# ---
		i0 := [
			[0.01]
			[0.01]];

		t0 := [
			[0.01]];

		# ---
		i1 := [
			[0.99]
			[0.99]];

		t1 := [
			[0.99]];

		# ---
		i2 := [
			[0.99]
			[0.01]];

		t2 :=[
			[0.01]];

		# ---
		i3 := [
			[0.01]
			[0.99]];

		t3 := [
			[0.01]];

		network := NeuralNetwork->New(input_nodes, hidden_nodes, output_nodes, learning_rate);

		each(i : iterations) {
			inputs : Float[,];	targets : Float[,];
			select(i % 4) {
				label 0: {
					inputs := i0;
					targets := t0;
				}

				label 1: {
					inputs := i1;
					targets := t1;
				}

				label 2: {
					inputs := i2;
					targets := t2;
				}

				label 3: {
					inputs := i3;
					targets := t3;
				}
			};
			
			network->Train(inputs, targets, learning_rate);
		};

		in := Float->New[2,1];
		in[0,0] := left ? 0.99 :0.01;
		in[1,0] := right ? 0.99 :0.01;
		
		out := network->Query(in);
		
 		Matrix2D->Divide(out, 0.5)->ToString()->Print();
	}
	
	New(input_nodes : Float, hidden_nodes : Float, output_nodes : Float, learning_rate : Float) {
		@input_nodes  := input_nodes;
		@hidden_nodes  := hidden_nodes;
		@output_nodes  := output_nodes;
		@learning_rate := learning_rate;

		@weight_inputs_hidden := Matrix2D->RandomNormal(0.01, Float->Pow(@input_nodes, -0.5), @hidden_nodes, @input_nodes);
		@weight_outputs_hidden := Matrix2D->RandomNormal(0.01, Float->Pow(@input_nodes, -0.5), @output_nodes, @hidden_nodes);
	}

	method : Query(inputs : Float[,]) ~ Float[,] {
		# calculate signals into hidden layer
		hidden_outputs := Matrix2D->DotSigmoid(@weight_inputs_hidden, inputs);
		# calculate the signals emerging from final output layer
		return Matrix2D->DotSigmoid(@weight_outputs_hidden, hidden_outputs);
	}

	method : Train(inputs : Float[,], targets : Float[,], rate : Float) ~ Nil {
		# calculate signals into hidden layer
# DebugMeta(@weight_inputs_hidden, inputs);		
        hidden_outputs := Matrix2D->DotSigmoid(@weight_inputs_hidden, inputs);
        # calculate signals into final output layer
# DebugMeta(@weight_outputs_hidden, hidden_outputs);        
        final_outputs  := Matrix2D->DotSigmoid(@weight_outputs_hidden, hidden_outputs);
        # output layer error is the (target - actual)
        output_errors := Matrix2D->Subtract(targets, final_outputs);
# DebugMeta(targets, final_outputs);        
        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes
		hidden_errors := Matrix2D->Dot(Matrix2D->Transpose(@weight_outputs_hidden), output_errors);
		# update the weights for the links between the input and hidden layers
		@weight_inputs_hidden := Matrix2D->Add(@weight_inputs_hidden, Adjust(rate, hidden_errors, hidden_outputs, inputs));
		# update the weights for the links between the hidden and output layers
		@weight_outputs_hidden := Matrix2D->Add(@weight_outputs_hidden, Adjust(rate, output_errors, final_outputs, hidden_outputs));		
	}

	method : Adjust(rate : Float, errors : Float[,], outputs : Float[,], inputs : Float[,]) ~ Float[,] {
		return Matrix2D->Multiple(rate, Matrix2D->Dot(Matrix2D->Multiple(errors, Matrix2D->Multiple(outputs, Matrix2D->Subtract(0.99, outputs))), Matrix2D->Transpose(inputs)));
	}

	function : DebugMeta(a : Float[,], b : Float[,]) ~ Nil {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];
		"A: [{$a_rows},{$a_cols}]"->PrintLine();

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];
		"B: [{$b_rows},{$b_cols}]"->PrintLine();

		"---"->PrintLine();
	}
}