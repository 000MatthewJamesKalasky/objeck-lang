class NeuralNetwork {
	@input_nodes : Float;
	@hidden_nodes : Float;
	@output_nodes : Float;
	@learning_rate : Float;
	@weight_inputs_hidden : Float[,];
	@weight_outputs_hidden : Float[,];

	function : Main(args : String[]) ~ Nil {
		input_nodes := 3;
		hidden_nodes := 3;
		output_nodes := 3;
		learning_rate := 0.3;

		inputs := [
			[1.0]
			[0.5]
			[-1.5]];

		# create instance of neural network
		n := NeuralNetwork->New(input_nodes,hidden_nodes,output_nodes, learning_rate);
		Show(n->Query(inputs));
	}
	
	New(input_nodes : Float, hidden_nodes : Float, output_nodes : Float, learning_rate : Float) {
		@input_nodes  := input_nodes;
		@hidden_nodes  := hidden_nodes;
		@output_nodes  := output_nodes;
		@learning_rate := learning_rate;

		@weight_inputs_hidden := Generate2D(0.0, Float->Power(@input_nodes, -0.5), @hidden_nodes, @input_nodes);
		@weight_outputs_hidden := Generate2D(0.0, Float->Power(@input_nodes, -0.5), @output_nodes, @input_nodes);
	}

	method : Query(inputs : Float[,]) ~ Float[,] {
		# calculate signals into hidden layer
		hidden_outputs := DotSigmoid(@weight_inputs_hidden, inputs);
		# calculate the signals emerging from final output layer
		return DotSigmoid(@weight_outputs_hidden, hidden_outputs);
	}

	method : Train(inputs : Float[,], targets : Float[,]) ~ Nil {
		# calculate signals into hidden layer
        hidden_outputs := DotSigmoid(@weight_inputs_hidden, inputs);
        # calculate signals into final output layer
        final_outputs  := DotSigmoid(@weight_outputs_hidden, hidden_outputs);

#~
		# output layer error is the (target - actual)
        output_errors = targets - final_outputs
        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes
        hidden_errors = numpy.dot(self.who.T, output_errors) 
        
        # update the weights for the links between the hidden and output layers
        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
        
        # update the weights for the links between the input and hidden layers
        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
~#
	}

	method : native : Transpose(a : Float[,]) ~ Float[,] {
		dim := a->Size();
		rows := dim[0];
		cols := dim[1];

		b := Float->New[cols, rows];
		for(r := 0; r < rows; r += 1;) {
			for(c := 0; c < cols; c += 1;) {
				b[c,r] := a[r,c];
			};
		};

		return b;
	}

	method : native : Subtract(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		if( a_rows <> b_rows | a_cols <> b_cols) {
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(i := 0; i < a_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := a[i,j] - b[i,j];
			};
		};

		return c;
	}

	method : native : Add(x : Float, b : Float[,]) ~ Float[,] {
		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		c := Float->New[b_rows, b_cols];
		for(i := 0; i < b_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := x - b[i,j];
			};
		};

		return c;
	}

	method : native : Multiple(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		if( a_rows <> b_rows | a_cols <> b_cols) {
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(i := 0; i < a_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := a[i,j] * b[i,j];
			};
		};

		return c;
	}

	method : native : DotSigmoid(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		if(a_cols <> b_rows) {
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(a_col := 0; a_col < a_rows; a_col += 1;) {
			for(b_col := 0; b_col < b_cols; b_col += 1;) {
				cx := 0.0;
				for(x_col := 0; x_col < b_rows; x_col += 1;) {
					cx += a[a_col, x_col] * b[x_col, b_col];
				};
				c[a_col, b_col] := Sigmoid(cx);
			};
		};

		return c;
	}

	method : native : Sigmoid(x : Float) ~ Float {
		return 1.0 / (1.0 + Float->Power(Float->E(), -1.0 * x));
	}

	method : Generate2D(mean : Float, variance : Float, rows : Int, cols : Int) ~ Float[,] {
		m := Float->New[rows, cols];

		for(i := 0; i < rows; i += 1;) {
			for(j := 0; j < cols; j += 1;) {
				m[i,j] := Generate(mean, variance);
			};
		};

		return m;
	}

	method : Generate(mean : Float, variance : Float) ~ Float {
		return  (-2.0 * variance * Float->Random()->Log())->SquareRoot() * (2.0 * Float->Pi() * Float->Random())->Cos() + mean;
	}

	method : Generate(n : Int) ~ Float[] {
		m := n + n % 2;
		values := Float->New[m];
 		
        for(i := 0; i < m; i += 2;) {
            x : Float;
            y : Float;
            rsq : Float;
            
            do {
                x := 2.0 * Float->Random() - 1.0;
                y := 2.0 * Float->Random() - 1.0;
                rsq := x * x + y * y;
            }
            while(rsq >= 1.0 | rsq = 0.0);
            
            f := (-2.0 * rsq->Log() / rsq)->SquareRoot();
            values[i] := x * f;
            values[i + 1] := y * f;
        };

    	return values;
    }

	function : Show(m : Float[,]) ~ Nil {
		dims := m->Size();
		rows := dims[0];
		cols := dims[1];

		for(r := 0; r < rows; r +=1;) {
			'['->Print();
			for(c := 0; c < cols; c +=1;) {
				m[r,c]->Print();
				if(c + 1 < cols) {
					", "->Print();
				}
				else {
					
				};
			};
			']'->PrintLine();
		};
	}
}