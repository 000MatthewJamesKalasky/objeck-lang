class NeuralNetwork {
	@input_nodes : Float;
	@hidden_nodes : Float;
	@output_nodes : Float;
	@learning_rate : Float;
	@weight_inputs_hidden : Float[,];
	@weight_outputs_hidden : Float[,];

	function : Main(args : String[]) ~ Nil {
		input_nodes := 2;
		hidden_nodes := 2;
		output_nodes := 1;
		learning_rate := 0.5;

		i0 :=[
			[0.0]
			[0.0]];

		t0 :=[
			[0.0]];

		i1 :=[
			[1.0]
			[1.0]];

		t1 :=[
			[1.0]];

		i2 :=[
			[1.0]
			[0.0]];

		t2 :=[
			[0.0]];

		i3 :=[
			[0.0]
			[1.0]];

		t3 :=[
			[0.0]];

		network := NeuralNetwork->New(input_nodes, hidden_nodes, output_nodes, learning_rate);

		for(i := 0; i < 5000; i += 1;) {
			inputs : Float[,];	targets : Float[,];
			select(i % 4) {
				label 0: {
					inputs := i0;
					targets := t0;
				}

				label 1: {
					inputs := i1;
					targets := t1;
				}

				label 2: {
					inputs := i2;
					targets := t2;
				}

				label 3: {
					inputs := i3;
					targets := t3;
				}
			};
			
			network->Train(inputs, targets, learning_rate);
		};

		in := [
			[1.0]
			[1.0]];
		out := network->Query(in);

		ToString(out)->Print();
	}
	
	New(input_nodes : Float, hidden_nodes : Float, output_nodes : Float, learning_rate : Float) {
		@input_nodes  := input_nodes;
		@hidden_nodes  := hidden_nodes;
		@output_nodes  := output_nodes;
		@learning_rate := learning_rate;

		@weight_inputs_hidden := RandomNormal(0.0, Float->Power(@input_nodes, -0.5), 
			@hidden_nodes, @input_nodes);
		@weight_outputs_hidden := RandomNormal(0.0, Float->Power(@input_nodes, -0.5), 
			@output_nodes, @input_nodes);
	}

	method : Query(inputs : Float[,]) ~ Float[,] {
		# calculate signals into hidden layer
		hidden_outputs := DotSigmoid(@weight_inputs_hidden, inputs);
		# calculate the signals emerging from final output layer
		return DotSigmoid(@weight_outputs_hidden, hidden_outputs);
	}

	method : Train(inputs : Float[,], targets : Float[,], learning_rate : Float) ~ Nil {
		# calculate signals into hidden layer
        hidden_outputs := DotSigmoid(@weight_inputs_hidden, inputs);
        # calculate signals into final output layer
        final_outputs  := DotSigmoid(@weight_outputs_hidden, hidden_outputs);
        # output layer error is the (target - actual)
        output_errors := Subtract(targets, final_outputs);
        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes
		hidden_errors := Dot(Transpose(@weight_outputs_hidden), output_errors);
		# update the weights for the links between the input and hidden layers
		@weight_inputs_hidden := Add(@weight_inputs_hidden, DeltaUpdate(learning_rate, hidden_errors, hidden_outputs, inputs));
		# update the weights for the links between the hidden and output layers
		@weight_outputs_hidden := Add(@weight_outputs_hidden, DeltaUpdate(learning_rate, output_errors, final_outputs, hidden_outputs));		
	}

	method : native : DeltaUpdate(rate : Float, errors : Float[,], outputs : Float[,], inputs : Float[,]) ~ Float[,] {
		return Multiple(rate, Dot(Multiple(errors, Multiple(outputs, Subtract(1.0, outputs))), Transpose(inputs)));
	}

	# ----------

	method : native : Add(x : Float, b : Float[,]) ~ Float[,] {
		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		c := Float->New[b_rows, b_cols];
		for(i := 0; i < b_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := x + b[i,j];
			};
		};

		return c;
	}

	method : native : Add(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		if( a_rows <> b_rows | a_cols <> b_cols) {
# "Add = A: [{$a_rows},{$a_cols}], B: [{$b_rows},{$b_cols}]"->PrintLine();		
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(i := 0; i < a_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := a[i,j] + b[i,j];
			};
		};

		return c;
	}

	method : native : Subtract(x : Float, b : Float[,]) ~ Float[,] {
		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		c := Float->New[b_rows, b_cols];
		for(i := 0; i < b_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := x - b[i,j];
			};
		};

		return c;
	}

	method : native : Subtract(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		if( a_rows <> b_rows | a_cols <> b_cols) {
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(i := 0; i < a_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := a[i,j] - b[i,j];
			};
		};

		return c;
	}

	method : native : Multiple(x : Float, b : Float[,]) ~ Float[,] {
		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		c := Float->New[b_rows, b_cols];
		for(i := 0; i < b_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := x * b[i,j];
			};
		};

		return c;
	}

	method : native : Multiple(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		if( a_rows <> b_rows | a_cols <> b_cols) {
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(i := 0; i < a_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := a[i,j] * b[i,j];
			};
		};

		return c;
	}

	method : native : Divide(x : Float, b : Float[,]) ~ Float[,] {
		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		c := Float->New[b_rows, b_cols];
		for(i := 0; i < b_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := x / b[i,j];
			};
		};

		return c;
	}

	method : native : Divide(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		if( a_rows <> b_rows | a_cols <> b_cols) {
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(i := 0; i < a_rows; i += 1;) {
			for(j := 0; j < b_cols; j += 1;) {
				c[i,j] := a[i,j] / b[i,j];
			};
		};

		return c;
	}

	method : native : Transpose(a : Float[,]) ~ Float[,] {
		dim := a->Size();
		rows := dim[0];
		cols := dim[1];

		b := Float->New[cols, rows];
		for(r := 0; r < rows; r += 1;) {
			for(c := 0; c < cols; c += 1;) {
				b[c,r] := a[r,c];
			};
		};

		return b;
	}

	function : native : Dot(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];
		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];

		if(a_cols <> b_rows & a_rows <> b_cols) {
# "Dot = A: [{$a_rows},{$a_cols}], B: [{$b_rows},{$b_cols}]"->PrintLine();		
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(a_col := 0; a_col < a_rows; a_col += 1;) {
			for(b_col := 0; b_col < b_cols; b_col += 1;) {
				cx := 0.0;
				for(x_col := 0; x_col < b_rows; x_col += 1;) {
					cx += a[a_col, x_col] * b[x_col, b_col];
				};
				c[a_col, b_col] := cx;
			};
		};

		return c;
	}

	method : native : Sigmoid(x : Float) ~ Float {
		return 1.0 / (1.0 + Float->Power(Float->E(), -1.0 * x));
	}

	method : native : DotSigmoid(a : Float[,], b : Float[,]) ~ Float[,] {
		a_dims := a->Size();
		a_rows := a_dims[0];
		a_cols := a_dims[1];

		b_dims := b->Size();
		b_rows := b_dims[0];
		b_cols := b_dims[1];


# "DotSigmoid = A: [{$a_rows},{$a_cols}], B: [{$b_rows},{$b_cols}]"->PrintLine();


		if(a_cols <> b_rows & a_rows <> b_cols) {
			return Nil;
		};

		c := Float->New[a_rows, b_cols];
		for(a_col := 0; a_col < a_rows; a_col += 1;) {
			for(b_col := 0; b_col < b_cols; b_col += 1;) {
				cx := 0.0;
				for(x_col := 0; x_col < b_rows; x_col += 1;) {
					cx += a[a_col, x_col] * b[x_col, b_col];
				};
				c[a_col, b_col] := Sigmoid(cx);
			};
		};

		return c;
	}

	method : RandomNormal(mean : Float, variance : Float, rows : Int, cols : Int) ~ Float[,] {
		m := Float->New[rows, cols];

		for(i := 0; i < rows; i += 1;) {
			for(j := 0; j < cols; j += 1;) {
				m[i,j] := RandomNormal(mean, variance);
			};
		};

		return m;
	}

	method : RandomNormal(mean : Float, variance : Float) ~ Float {
		return  (-2.0 * variance * Float->Random()->Log())->SquareRoot() * (2.0 * Float->Pi() * Float->Random())->Cos() + mean;
	}

	method : RandomNormal(n : Int) ~ Float[] {
		m := n + n % 2;
		values := Float->New[m];
 		
        for(i := 0; i < m; i += 2;) {
            x : Float;
            y : Float;
            rsq : Float;
            
            do {
                x := 2.0 * Float->Random() - 1.0;
                y := 2.0 * Float->Random() - 1.0;
                rsq := x * x + y * y;
            }
            while(rsq >= 1.0 | rsq = 0.0);
            
            f := (-2.0 * rsq->Log() / rsq)->SquareRoot();
            values[i] := x * f;
            values[i + 1] := y * f;
        };

    	return values;
    }

	function : ToString(m : Float[,]) ~ String {
		buffer := "";

		dims := m->Size();
		rows := dims[0];
		cols := dims[1];

		for(r := 0; r < rows; r +=1;) {
			buffer->Append('[');
			for(c := 0; c < cols; c +=1;) {
				buffer->Append(m[r,c]);
				if(c + 1 < cols) {
					buffer->Append(", ");
				};
			};
			buffer->Append("]\n");
		};

		return buffer;
	}
}