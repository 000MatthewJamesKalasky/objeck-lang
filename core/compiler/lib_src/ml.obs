use Collection;
use Data.CSV;

#~
Provides support for machine learning
~#
bundle System.ML {
	#~
	2D matrix operations
	~#
	class Matrix2D {
		#~
		Adds a constant to a matrix
		@param x constant to add
		@param b matrix
		@return updated matrix
		~#
		function : native : Add(x : Float, b : Float[,]) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := x + b[i,j];
				};
			};

			return c;
		}

		#~
		Adds a constant to a matrix
		@param b matrix
		@param x constant to add
		@return updated matrix
		~#
		function : native : Add(b : Float[,], x : Float) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := b[i,j] + x;
				};
			};

			return c;
		}
		
		#~
		Adds two matrices
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Add(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			if( a_rows <> b_rows | a_cols <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(i := 0; i < a_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := a[i,j] + b[i,j];
				};
			};

			return c;
		}

		#~
		Subtracts a constant from a matrix
		@param x constant to subtract
		@param b matrix
		@return updated matrix
		~#
		function : native : Subtract(x : Float, b : Float[,]) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := x - b[i,j];
				};
			};

			return c;
		}
		
		#~
		Adds a constant to a matrix
		@param b matrix
		@param x constant to add
		@return updated matrix
		~#
		function : native : Subtract(b : Float[,], x : Float) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := b[i,j] - x;
				};
			};

			return c;
		}

		#~
		Subtracts two matrices
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Subtract(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			if( a_rows <> b_rows | a_cols <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(i := 0; i < a_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := a[i,j] - b[i,j];
				};
			};

			return c;
		}

		#~
		Multiplies a constant by a matrix
		@param x constant to multiple
		@param b matrix
		@return updated matrix
		~#
		function : native : Multiple(x : Float, b : Float[,]) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := x * b[i,j];
				};
			};

			return c;
		}

		#~
		Multiplies a constant by a matrix
		@param b matrix
		@param x constant to multiple
		@return updated matrix
		~#
		function : native : Multiple(b : Float[,], x : Float) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := b[i,j] * x;
				};
			};

			return c;
		}
		
		#~
		Multiplies two matrices using the Hadamard rule
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Multiple(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			if( a_rows <> b_rows | a_cols <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(i := 0; i < a_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := a[i,j] * b[i,j];
				};
			};

			return c;
		}

		#~
		Divides a constant by a matrix
		@param b matrix
		@param x constant to divide
		@return updated matrix
		~#
		function : native : Divide(b : Float[,], x : Float) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := b[i,j] / x;
				};
			};

			return c;
		}

		#~
		Divides a constant by a matrix
		@param x constant to divide
		@param b matrix
		@return updated matrix
		~#
		function : native : Divide(x : Float, b : Float[,]) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := b[i,j] / x;
				};
			};

			return c;
		}

		#~
		Divides two matrices
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Divide(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			if( a_rows <> b_rows | a_cols <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(i := 0; i < a_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := a[i,j] / b[i,j];
				};
			};

			return c;
		}

		#~
		Transpose a matrix swapping rows and columns.
		@param a matrix
		@return transposed matrix
		~#
		function : native : Transpose(a : Float[,]) ~ Float[,] {
			dim := a->Size();
			rows := dim[0];
			cols := dim[1];

			b := Float->New[cols, rows];
			for(r := 0; r < rows; r += 1;) {
				for(c := 0; c < cols; c += 1;) {
					b[c,r] := a[r,c];
				};
			};

			return b;
		}

		#~
		Calculates the dot product.
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Dot(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			if(a_cols <> b_rows & a_rows <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(a_col := 0; a_col < a_rows; a_col += 1;) {
				for(b_col := 0; b_col < b_cols; b_col += 1;) {
					cx := 0.0;
					for(x_col := 0; x_col < b_rows; x_col += 1;) {
						cx += a[a_col, x_col] * b[x_col, b_col];
					};
					c[a_col, b_col] := cx;
				};
			};

			return c;
		}

		#~
		Sigmoid 'S' function
		@param x input value
		@return Sigmoid value
		~#
		function : native : Sigmoid(x : Float) ~ Float {
			return 1.0 / (1.0 + Float->Pow(Float->E(), -1.0 * x));
		}

		#~
		Applies the Sigmoid function to all elements
		@param b matrix
		@return updated matrix
		~#
		function : native : Sigmoid(b : Float[,]) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := Sigmoid(b[i,j]);
				};
			};

			return c;
		}
		
		#~
		Calculates the Dot Product applying while applying the Sigmoid function to all elements
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : DotSigmoid(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];
			
			if(a_cols <> b_rows & a_rows <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(a_col := 0; a_col < a_rows; a_col += 1;) {
				for(b_col := 0; b_col < b_cols; b_col += 1;) {
					cx := 0.0;
					for(x_col := 0; x_col < b_rows; x_col += 1;) {
						cx += a[a_col, x_col] * b[x_col, b_col];
					};
					c[a_col, b_col] := Sigmoid(cx);
				};
			};

			return c;
		}

		#~
		Generates a random 2D array of values from 0.0 to 1.0
		@param rows rows
		@param cols columns
		@return updated matrix
		~#
		function : Random(rows : Int, cols : Int) ~ Float[,] {
			m := Float->New[rows, cols];

			for(i := 0; i < rows; i += 1;) {
				for(j := 0; j < cols; j += 1;) {
					m[i,j] := Float->Random();
				};
			};
			
			return m;
		}
		
		#~
		Generates a random normal distribution of values
		@param mean center of values
		@param variance variance in values
		@param rows rows
		@param cols columns
		@return updated matrix
		~#
		function : RandomNormal(mean : Float, variance : Float, rows : Int, cols : Int) ~ Float[,] {
			m := Float->New[rows, cols];

			for(i := 0; i < rows; i += 1;) {
				for(j := 0; j < cols; j += 1;) {
					m[i,j] := RandomNormal(mean, variance);
				};
			};

			return m;
		}

		#~
		Generates a random normal value
		@param mean center of values
		@param variance variance in values
		@return updated matrix
		~#
		function : RandomNormal(mean : Float, variance : Float) ~ Float {
			return  (-2.0 * variance * Float->Random()->Log())->Sqrt() * (2.0 * Float->Pi() * Float->Random())->Cos() + mean;
		}

		#~
		Splits a matrix
		@param b matrix
		@param offset offset index
		@param count number of rows to split
		@param is_row true for row split, false for column
		@return copied matrix
		~#
		function : native : Split(b : Float[,], offset : Int, count : Int, is_row : Bool) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];
			
			c : Float[,];
			if(is_row) {
				if(count + offset <= 0 | count + offset > b_rows) {
					return Nil;
				};

				c := Float->New[count, b_cols];
				count := count + offset;
				for(row := offset; row < count; row += 1;) {
					for(col := 0; col < b_cols; col += 1;) {
						c[row - offset, col] := b[row, col];
					};
				};
			}
			else {
				if(count + offset <= 0 | count + offset > b_cols) {
					return Nil;
				};

				c := Float->New[b_rows, count];
				count := count + offset;
				for(row := 0; row < b_rows; row += 1;) {
					for(col := offset; col < count; col += 1;) {
						c[row, col - offset] := b[row, col];
					};
				};
			};

			return c;
		}
		
		#~
		Concatenates two matrix
		@param a left matrix
		@param b right matrix
		@param is_row true concatenate by rows, false for columns
		@return concatenated matrix
		~#
		function : Concatenate(a : Float[,], b : Float[,], is_row : Bool) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c : Float[,];
			if(is_row) {
				if(a_cols <> b_cols) {
					return Nil;
				};

				c_rows := a_rows + b_rows;
				c := Float->New[c_rows, a_cols];

				for(row := 0; row < a_rows; row += 1;) {
					for(col := 0; col < a_cols; col += 1;) {
						c[row, col] := a[row, col];
					};
				};

				for(row := 0; row < b_rows; row += 1;) {
					for(col := 0; col < b_cols; col += 1;) {
						c[row + a_rows, col] := b[row, col];
					};
				};
			}
			else {
				if(a_rows <> b_rows) {
					return Nil;
				};

				c_cols := a_cols + b_cols;
				c := Float->New[a_rows, c_cols];

				for(row := 0; row < a_rows; row += 1;) {
					for(col := 0; col < a_cols; col += 1;) {
						c[row, col] := a[row, col];
					};
				};

				for(row := 0; row < b_rows; row += 1;) {
					for(col := 0; col < b_cols; col += 1;) {
						c[row, col + a_cols] := b[row, col];
					};
				};
			};

			return c;
		}
	}

	#~
	Boolean matrix reference
	~#
	class BoolMatrixRef {
		@value : Bool[,];

		#~
		Default constructor
		~#
		New() {
			Parent();
		}

		#~
		Copy constructor
		@param value boolean value
		~#
		New(value : Bool[,]) {
			Parent();
			@value := value;
		}

		#~
		Get boolean value
		@return boolean value
		~#
		method : public : Get() ~ Bool[,] {
			return @value;
		}

		#~
		Formats the matrix into a string
		@return string value
		~#
		method : public : ToString() ~ String {
			return @value->ToString();
		}
		
		#~
		Splits a 2D boolean matrix
		@param target_offset index of first column with target data
		@param matrix matrix to split
		@return split matrix
		~#
		method : public : Split(training_offset : Float) ~ BoolMatrixRef[] {
			dims := @value->Size();
			input_rows := dims[0];
			input_cols := dims[1];

			training_rows := (input_rows->As(Float) * training_offset)->As(Int);
			training_matrix := Bool->New[training_rows, input_cols];
			dims := training_matrix->Size();
			train_rows := dims[0];

			each(i : train_rows) {
				each(j : input_cols) {
					training_matrix[i, j] := @value[i, j];
				};
			};


			data_rows := input_rows - training_rows;
			data_matrix := Bool->New[data_rows, input_cols];
			dims := data_matrix->Size();
			data_rows := dims[0];

			each(i : data_rows) {
				each(j : input_cols) {
					data_matrix[i, j] := @value[training_rows, j];
				};
				training_rows += 1;
			};

			outputs := BoolMatrixRef->New[2];
			outputs[0] := BoolMatrixRef->New(training_matrix);
			outputs[1] := BoolMatrixRef->New(data_matrix);

			return outputs;
		}
	}

	#~
	Float matrix reference with convenience methods
	~#
	class FloatMatrixRef {
		@value : Float[,];

		#~
		Default constructor
		~#
		New() {
			Parent();
		}

		#~
		Copy constructor
		@param value boolean value
		~#
		New(value : Float[,]) {
			Parent();
			@value := value;
		}

		#~
		Get float value
		@return float value
		~#
		method : public : Get() ~ Float[,] {
			return @value;
		}

		method : GetBoolen() ~ Bool {
			dim := @value->Size();
			if(dim->Size() = 2 & dim[0] = 1 & dim[1] = 1) {
				return @value[0,0] = 1.0;
			}
			
			return false;
		}
		
		#~
		Set boolean value
		@param value boolean value		
		~#
		method : public : Set(value : Float[,]) ~ Nil {
			@value := value;
		}

		#~
		Formats the matrix into a string
		@return string value
		~#
		method : public : ToString() ~ String {
			return @value->ToString();
		}

		#~
		Parsers a boolean value
		@return boolean value
		~#
		method : public : ToBool() ~ Bool {
			return ToInt() <> 0;
		}

		#~
		Parsers an integer value
		@return integer value
		~#
		method : public : ToInt() ~ Int {
			return Float->Round(ToFloat());
		}

		#~
		Parsers a decimal value
		@return decimal value
		~#
		method : public : ToFloat() ~ Float {
			dim := @value->Size();
			if(dim->Size() = 2 & dim[0] = 1 & dim[1] = 1) {
				return @value[0,0];
			};

			return 0.0;
		}
	}

	#~
	Simple neural network. Input values should be scaled to between 0.0 to 1.0. 
	The tuned network should return outputs between 0.0 to 1.0.

	```
network : NeuralNetwork;
filename := "data/model.dat";
inputs_targets := MatrixReader->LoadSplitMatrices(args[0], 1, 0.8); # 20% test data

# load model
if(args->Size() = 2) {
   network := NeuralNetwork->Load(filename);
   "Loaded model..."->PrintLine();

   "Testing model..."->PrintLine();
   tests := inputs_targets[2];
   answers := inputs_targets[3];

   failures := 0;
   each(i : answers) {
      answer := answers->Get(i)->ToBool();
      predict := network->Query(FloatMatrixRef->New(tests->Get(i)->Get()));
      if(predict <> answer) {
         failures += 1;
      };
   };

   correct := 100.0 * (1.0 - failures->As(Float) / tests->Size()->As(Float));
   System.IO.Standard->Print("Tests: ")->Print(tests->Size())->Print(", correct: ")->SetFloatPrecision(5)->Print(correct)->PrintLine("%");
}
# train and store model
else if(args->Size() = 1) {
   "Training model..."->PrintLine();
   network := NeuralNetwork->Train(2, inputs_targets[0], 8, 1, inputs_targets[1], 0.01725, 256);
   if(inputs_targets <> Nil) {
      network->Store(filename);
      "Stored model..."->PrintLine();
   };
}
	```
	~#
	class NeuralNetwork {
		@input_nodes : Float;
		@hidden_nodes : Float;
		@output_nodes : Float;
		@learning_rate : Float;
		@weight_inputs_hidden : Float[,];
		@weight_outputs_hidden : Float[,];
		@threshold : Float;
		@attempts : Int;

		#~
		Trains the network
		@param input_nodes number of input nodes
		@param inputs : training inputs
		@param hidden_factor size of hidden layer, factor of input (i.e. input_nodes * hidden_factor)
		@param output_nodes training outputs
		@param targets training targets
		@param learning_rate learning rate
		@param iterations number of training iterations
		~#
		function : Train(input_nodes : Int, inputs : Vector<FloatMatrixRef>, hidden_factor : Int, output_nodes : Int, targets : Vector<FloatMatrixRef>, learning_rate : Float, iterations : Int) ~ NeuralNetwork {
			network := NeuralNetwork->New(input_nodes, hidden_factor * input_nodes, output_nodes, learning_rate);
			network->Train(inputs, targets, learning_rate, iterations);
			return network;		
		}
		
		New : private (input_nodes : Float, hidden_nodes : Float, output_nodes : Float, learning_rate : Float) {
			@input_nodes := input_nodes;
			@hidden_nodes := hidden_nodes;
			@output_nodes := output_nodes;
			@learning_rate := learning_rate;
			
			@threshold := 0.80;
			@attempts := 10;

			@weight_inputs_hidden := Matrix2D->RandomNormal(0.0, Float->Pow(@input_nodes, -1.0), @hidden_nodes, @input_nodes);
			@weight_outputs_hidden := Matrix2D->RandomNormal(0.0, Float->Pow(@input_nodes, -1.0), @output_nodes, @hidden_nodes);
		}

		New : private (weight_inputs_hidden : Float[,], weight_outputs_hidden : Float[,]) {
			@weight_inputs_hidden := weight_inputs_hidden;
			@weight_outputs_hidden := weight_outputs_hidden;

			@threshold := 0.80;
			@attempts := 10;
		}

		#~
		Sets the activation threshold. Activation if output > threshold or < 1.0 - threshold
		@param threshold activation threshold default is 0.85
		@param attempts number of activation attempts
		~#
		method : public : SetActivation(threshold : Float, attempts : Int) ~ Nil {
			@threshold := threshold;
			@attempts := attempts;
		}

		#~
		Query the network
		@param inputs query inputs
		@return true if activated, false otherwise
		~#
		method : public : Query(inputs : FloatMatrixRef) ~ Bool {
			# try and fail is unsuccessful
			each(i : @attempts) {
				result := Confidence(inputs);
				if(result > @threshold) {
					return true;
				};		
			};

			return false;
		}

		#~
		Query the network's confidence for the give inputs
		@param inputs query inputs
		@return confidence percentage
		~#
		method : public : Confidence(inputs : FloatMatrixRef) ~ Float {
			outputs := Query(inputs->Get());
			if(outputs <> Nil) {
				dims := outputs->Size();
				if(dims->Size() = 2 & dims[0] = 1 & dims[1] = 1) {
					return outputs[0,0];
				};
			};

			return 0.0
		}

		#~
		Loads network inputs and outputs
		@param filename file to store to
		@return true if successful, false otherwise
		~#
		function : Load(filename : String) ~ NeuralNetwork {
			data := System.IO.Filesystem.FileReader->ReadBinaryFile(filename);
			if(data <> Nil) {
				# read inputs weight
				deserializer := System.IO.Deserializer->New(data);
				height := deserializer->ReadInt();
				width := deserializer->ReadInt();

				weight_inputs_hidden := Float->New[height, width];
				each(i : height) {
					each(j : width) {
						weight_inputs_hidden[i,j] := deserializer->ReadFloat();
					};
				};
# "--- Read A ---"->PrintLine();
# weight_inputs_hidden->ToString()->PrintLine();				
							
				height := deserializer->ReadInt();
				width := deserializer->ReadInt();

				weight_outputs_hidden := Float->New[height, width];
				each(i : height) {
					each(j : width) {
						weight_outputs_hidden[i,j] := deserializer->ReadFloat();
					};
				};
# "--- Read B ---"->PrintLine();
# weight_outputs_hidden->ToString()->PrintLine();

				return NeuralNetwork->New(weight_inputs_hidden, weight_outputs_hidden);
			}

			return Nil;
		}

		#~
		Saves final network inputs and outputs
		@param filename file to store to
		@return true if successful, false otherwise
		~#
		method : public : Store(filename : String) ~ Bool {
			input_dims := @weight_inputs_hidden->Size();
			if((input_dims->Size() = 2 & input_dims[0] > 0 & input_dims[1] > 0) = false) {
				return false;
			}

			# write inputs weight
			height := input_dims[0];
			width := input_dims[1];

			serializer := System.IO.Serializer->New();

			serializer->Write(height);
			serializer->Write(width);

			each(i : height) {
				each(j : width) {
					serializer->Write(@weight_inputs_hidden[i,j])
				};
			};
# "--- Write A ---"->PrintLine();
# @weight_inputs_hidden->ToString()->PrintLine();

			# write outputs weight
			output_dims := @weight_outputs_hidden->Size();
			if((output_dims->Size() = 2 & output_dims[0] > 0 & output_dims[1] > 0) = false) {
				return false;
			}

			# write outputs weight
			height := output_dims[0];
			width := output_dims[1];

			serializer->Write(height);
			serializer->Write(width);

			each(i : height) {
				each(j : width) {
					serializer->Write(@weight_outputs_hidden[i,j])
				};
			};
# "--- Write B ---"->PrintLine();
# @weight_outputs_hidden->ToString()->PrintLine();

			return System.IO.Filesystem.FileWriter->WriteFile(filename, serializer->Serialize());
		}

		method : Query(inputs : Float[,]) ~ Float[,] {
			# calculate signals into hidden layer
			hidden_outputs := Matrix2D->DotSigmoid(@weight_inputs_hidden, inputs);
			# calculate the signals emerging from final output layer
			return Matrix2D->DotSigmoid(@weight_outputs_hidden, hidden_outputs);
		}

		method : Train(inputs : Vector<FloatMatrixRef>, targets : Vector<FloatMatrixRef>, rate : Float, iterations : Int) ~ Nil {
			if(inputs->Size() = targets->Size()) {
				each(i : iterations) {
					each(j : inputs) {
						input := inputs->Get(j)->Get();
						target := targets->Get(j)->Get();

						# calculate signals into hidden layer
						hidden_outputs := Matrix2D->DotSigmoid(@weight_inputs_hidden, input);
						# calculate signals into final output layer
						final_outputs  := Matrix2D->DotSigmoid(@weight_outputs_hidden, hidden_outputs);
						# output layer error is the (target - actual)
						output_errors := Matrix2D->Subtract(target, final_outputs);
						# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
						hidden_errors := Matrix2D->Dot(Matrix2D->Transpose(@weight_outputs_hidden), output_errors);
						# update the weights for the links between the input and hidden layers
						@weight_inputs_hidden := Matrix2D->Add(@weight_inputs_hidden, Adjust(rate, hidden_errors, hidden_outputs, input));
						# update the weights for the links between the hidden and output layers
						@weight_outputs_hidden := Matrix2D->Add(@weight_outputs_hidden, Adjust(rate, output_errors, final_outputs, hidden_outputs));
					};
				};
			};
		}

		method : Adjust(rate : Float, errors : Float[,], outputs : Float[,], inputs : Float[,]) ~ Float[,] {
			return Matrix2D->Multiple(rate, Matrix2D->Dot(Matrix2D->Multiple(errors, Matrix2D->Multiple(outputs, Matrix2D->Subtract(1.0, outputs))), Matrix2D->Transpose(inputs)));
		}
	}

	#~
	Utilities for reading data from CSV data sources 
	~#
	class MatrixReader {
		#~
		Load input and output data into matrices
		@param filename file to process
		@param target_offset index of first column with target data
		@return input and output matrix data
		~#
		function : public : LoadMatrices(filename : String, target_offset : Int) ~ Vector[]<FloatMatrixRef> {
			table := CsvTable->New(System.IO.Filesystem.FileReader->ReadFile(filename));
			if(table <> Nil & table->Size() > 0) {
				inputs := Vector->New()<FloatMatrixRef>;
				targets := Vector->New()<FloatMatrixRef>;

				each(row := table) {
					input_length := row->Size() - target_offset;
					target_length := row->Size() - input_length;
					if(input_length < 0) {
						return Nil;
					};

					input_array := Float->New[input_length, 1];
					target_array := Float->New[target_length, 1];

					each(i : input_length) {
						input_array[i, 0] := row->Get(i)->ToFloat();
					};

					target_index := 0;
					for(i := input_length; i < row->Size(); i += 1;) {
						target_array[target_index, 0] := row->Get(i)->ToFloat();
					};

					inputs->AddBack(FloatMatrixRef->New(input_array));
					targets->AddBack(FloatMatrixRef->New(target_array));
				};

				input_targets := Vector->New[2]<FloatMatrixRef>;
				input_targets[0] := inputs;
				input_targets[1] := targets;

				return input_targets;
			};

			return Nil;
		}

		#~
		Load input and output data into split matrices for training and testing
		@param filename file to process
		@param target_offset index of first column with output data
		@param training_perc percentage of data used for training
		@return split matrices for training and testing, first two matrices are training, latter two test
		~#
		function : public : LoadSplitMatrices(filename : String, target_offset : Int, training_perc : Float) ~ Vector[]<FloatMatrixRef> {
			table := CsvTable->New(System.IO.Filesystem.FileReader->ReadFile(filename));
			if(table <> Nil & table->Size() > 0) {
				train_count := (table->Size()->As(Float) * training_perc)->As(Int);
				if(train_count < 1) {
					return Nil;
				};

				training_inputs := Vector->New()<FloatMatrixRef>;
				training_targets := Vector->New()<FloatMatrixRef>;

				test_inputs := Vector->New()<FloatMatrixRef>;
				test_targets := Vector->New()<FloatMatrixRef>;

				count := 0;
				each(row := table) {
					input_length := row->Size() - target_offset;
					target_length := row->Size() - input_length;
					if(input_length < 0) {
						return Nil;
					};

					input_array := Float->New[input_length, 1];
					target_array := Float->New[target_length, 1];

					each(i : input_length) {
						input_array[i, 0] := row->Get(i)->ToFloat();
					};

					target_count := 0;
					for(i := input_length; i < row->Size(); i += 1;) {
						target_array[target_count, 0] := row->Get(i)->ToFloat();
					};

					if(count < train_count) {
						training_inputs->AddBack(FloatMatrixRef->New(input_array));
						training_targets->AddBack(FloatMatrixRef->New(target_array));
					}
					else {
						test_inputs->AddBack(FloatMatrixRef->New(input_array));
						test_targets->AddBack(FloatMatrixRef->New(target_array));
					};

					count += 1;
				};

				input_training_targets := Vector->New[4]<FloatMatrixRef>;
				input_training_targets[0] := training_inputs;
				input_training_targets[1] := training_targets;
				input_training_targets[2] := test_inputs;
				input_training_targets[3] := test_targets;

				return input_training_targets;
			};

			return Nil;
		}
	}

	#~
	Binary decision tree algorithm
	
```
input := [
	[true, false, true]
	[true, false, true]
	[true, true, true]
	[true, true, true]
	[true, true, true]
	[true, true, true]
	[true, true, true]
	[true, true, true]
	[false, true, true]
	[false, true, true]
	[true, true, false]
	[true, true, false]
	[false, true, false]
	[false, true, false]
	[false, true, false]
	[false, true, false]
	[false, false, false]
	[false, false, false]
	[false, false, false]
	[false, false, false]
];
"--- Dataset ---"->PrintLine();
input->ToString()->PrintLine()

training := Bool->Split(0.25, input)
"\n--- Training ---"->PrintLine();
training->ToString()->PrintLine()

output := DecisionTree->New(input)->Train()
"\n--- Split ---"->PrintLine();
output->ToString()->PrintLine()

cols := training->Columns()->As(Float)
training_matches := DecisionTree->Matches(cols - 1,output)->As(Float)
"\n--- Confidence ---"->PrintLine();
(output <> input)->PrintLine()
(training->Rows()->As(Float) / training_matches)->PrintLine()
```
	~#
	class DecisionTree {
		@input_matrix : Bool[,];
		@output_matrix : Bool[,];
		@decisions : Vector<Decision>;

		#~
		Constructor
		@param matrix boolean input training matrix, last row is the result
		~#
		New(matrix : Bool[,]) {
			@input_matrix := matrix;
		}

		#~
		Get the output matrix
		@return output matrix
		~#
		method : public : GetOutput() ~ Bool[,] {
			return @output_matrix;
		}

		#~
		Calculates a list of decision splits
		@return list of decision tree splits
		~#
		method : public : Step() ~ Vector<Decision> {					
			decisions := Vector->New()<Decision>;
			dims := @input_matrix->Size();
			
			cols := dims[1] - 1;
			if(cols > 0) {
				done := false;
				while(<>done) {
					best_wgi := 1000000.0;
					best_index := -1;

					each(i : cols) {
						wgi := WeightedGini(i, cols);
						if(wgi <> 0.0 & wgi < best_wgi) {
							best_wgi := wgi;
							best_index := i;
						}
					}

					if(best_index > - 1) {
						@input_matrix := Split(best_index, @input_matrix);
						decisions->AddBack(Decision->New(best_index, best_wgi));

					}
					else {
						done := true;
					};
				};
			};

			return decisions;
		}


		#~
		Splits a matrix along the decisions columns
		@return split matrix
		~#
		method : public : Train() ~ Bool {
			decisions := Step();
			if(decisions <> Nil) {
				in := @input_matrix;

				each(decision := decisions) {
					in := DecisionTree->Split(decision->GetIndex(), in);
				};
				@output_matrix := in;

				return true;
			};

			return false;
		}

		#~
		Splits a matrix along the given column
		@param index index used to split
		@param in matrix to split
		@return split matrix
		~#
		function : Split(index : Int, in : Bool[,]) ~ Bool[,] {
			split_len := Matches(index, in);
			if(split_len > 0) {
				dims := in->Size();
				rows := dims[0];
				cols := dims[1];

				out := Bool->New[split_len, cols];
				count := 0;
				if(index > -1 & index < rows) {
					each(row : rows) {
						if(in[row, index]) {
							each(k : cols) {
								out[count, k] := in[row, k];
							};
							count += 1;
						};
					};
				};

				return out;
			};

			return Nil;
		}

		#~
		Calculates the Gini index
		@param acheived number acheived
		@param goal achievement target
		@return Gini index
		~#
		function : native : Gini(acheived : Float, goal : Float) ~ Float {
			reached := acheived / goal;
			not_reached := (goal - acheived) / goal;
			return 1 - (reached*reached + not_reached*not_reached);
		}

		method : native : WeightedGini(test : Int, target : Int) ~ Float {
			wgi := 0.0;

			dims := @input_matrix->Size();
			size := dims[0]->As(Float);

			# left
			goal := Matches(test, @input_matrix);
			if(goal > 0.0) {
				wgi += goal / size * Gini(Acheived(test, target, @input_matrix), goal);
			}
			else {
				return 0.0;
			};

			
			# right
			goal := Mismatches(test, @input_matrix);
			if(goal > 0.0) {
				wgi += goal / size * Gini(Unacheived(test, target, @input_matrix), goal);
			}
			else {
				return 0.0;
			};

			return wgi;
		}

		#~
		Count matches in a column
		@param index column index
		@param matrix matrix to inspect
		@return number of matches
		~#
		function : Matches(index : Int, matrix : Bool[,]) ~ Int {
			dims := matrix->Size();
			rows := dims[0];

			count := 0;
			if(index > -1 & index < rows) {
				each(row : rows) {
					if(matrix[row, index]) {
						count += 1;
					};
				};
			};

			return count;
		}

		#~
		Count mismatches in a column
		@param index column index
		@param matrix matrix to inspect
		@return number of mismatches
		~#
		function : Mismatches(index : Int, matrix : Bool[,]) ~ Int {
			dims := matrix->Size();
			rows := dims[0];

			count := 0;
			if(index > -1 & index < rows) {
				each(row : rows) {
					if(matrix[row, index] = false) {
						count += 1;
					};
				};
			};

			return count;
		}

		method : native : Acheived(test_index : Int, goal_index : Int, matrix : Bool[,]) ~ Float {
			dims := matrix->Size();
			rows := dims[0];

			count := 0;
			if(test_index > -1 & test_index < rows & goal_index > -1 & goal_index < rows) {
				each(row : rows) {
					if(matrix[row, test_index] & matrix[row, goal_index]) {
						count += 1;
					};
				};
			};

			return count;
		}

		method : native : Unacheived(test_index : Int, goal_index : Int, matrix : Bool[,]) ~ Float {
			dims := matrix->Size();
			rows := dims[0];

			count := 0;
			if(test_index > -1 & test_index < rows & goal_index > -1 & goal_index < rows) {
				each(row : rows) {
					if(matrix[row, test_index] = false & matrix[row, goal_index]) {
						count += 1;
					};
				};
			};

			return count;
		}

		#~
		Loads a boolean matrix from a CSV file of 1s and 0s
		@param filename CSV file to load
		@return boolean matrix
		~#
		function : LoadMatrix(filename : String) ~ Bool[,] {
			table := CsvTable->New(System.IO.Filesystem.FileReader->ReadFile(filename));
			if(table->IsParsed()) {
				row_size := table->Size();
				if(row_size->Size() > 1) {
					column := table->Get(1);
					column_size := column->Size();

					matrix := Bool->New[row_size - 1, column_size];
					for(i := 1; i < row_size; i += 1;) {
						each(j : column_size) {
							value := table->Get(i)->Get(j)->ToLower();
							if(value->Equals(["0", "o", "0.0", "false"])) {
								matrix[i - 1, j] := false;
							}
							else {
								matrix[i - 1, j] := true;	
							};
						};
					};

					return matrix;
				};
			};

			return Nil;
		}
	}

	#~
	Decision tree node
	~#
	class Decision {
		@index : Int;
		@weighted_gini : Float;
		
		New(index : Int, weighted_gini : Float) {
			@index := index;
			@weighted_gini := weighted_gini;
		}

		#~
		Gets the weighted Gini value
		@return weighted Gini value
		~#
		method : public : GetWeightedGini() ~ Float {
			return @weighted_gini;
		}

		#~
		Gets the test index
		@return test index
		~#
		method : public : GetIndex() ~ Int {
			return @index;
		}

		#~
		Returns the string representation 
		@return string representation 
		~#
		method : public : ToString() ~ String {
			return "index={$@index},weighted_gini={$@weighted_gini}";
		}
	}
}
