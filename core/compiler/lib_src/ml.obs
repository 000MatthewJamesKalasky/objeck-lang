use Collection;
use Data.CSV;

#~
Provides support for machine learning
~#
bundle System.ML {
	#~
	2D matrix operations
	~#

	class Proxy {
		@lib_proxy : static : System.API.DllProxy;
		
		function : GetDllProxy() ~ System.API.DllProxy {
			if(@lib_proxy = Nil) {
				@lib_proxy := System.API.DllProxy->New("libobjk_ml");
			};

			return @lib_proxy;
		}
	}

	#
	# TOOD: replace with Eigen library class (https://eigen.tuxfamily.org/)
	#
	class Matrix2D {
		#~
		Adds a constant to a matrix
		@param x constant to add
		@param b matrix
		@return updated matrix
		~#
		function : native : Add(x : Float, b : Float[,]) ~ Float[,] {
			if(b <> Nil) {				
				array_args := Base->New[3];
				array_args[0] := IntRef->New();
				array_args[1] := FloatRef->New(x);				
				array_args[2] := FloatMatrixRef->New(b);				
				@lib_proxy := Proxy->GetDllProxy();
				@lib_proxy->CallFunction("ml_matrix_add_sm", array_args);
				
				holder := array_args[0]->As(FloatMatrixRef);
				return holder->Get();
			};
			
			return Nil;

#~			
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := x + b[i,j];
				};
			};

			return c;
~#			
		}

		#~
		Adds a constant to a matrix
		@param b matrix
		@param x constant to add
		@return updated matrix
		~#
		function : native : Add(b : Float[,], x : Float) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := b[i,j] + x;
				};
			};

			return c;
		}
		
		#~
		Adds two matrices
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Add(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			if(a_rows <> b_rows | a_cols <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(i := 0; i < a_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := a[i,j] + b[i,j];
				};
			};

			return c;
		}

		#~
		Subtracts a constant from a matrix
		@param x constant to subtract
		@param b matrix
		@return updated matrix
		~#
		function : native : Subtract(x : Float, b : Float[,]) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := x - b[i,j];
				};
			};

			return c;
		}
		
		#~
		Adds a constant to a matrix
		@param b matrix
		@param x constant to add
		@return updated matrix
		~#
		function : native : Subtract(b : Float[,], x : Float) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := b[i,j] - x;
				};
			};

			return c;
		}

		#~
		Subtracts two matrices
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Subtract(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			if( a_rows <> b_rows | a_cols <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(i := 0; i < a_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := a[i,j] - b[i,j];
				};
			};

			return c;
		}

		#~
		Multiplies a constant by a matrix
		@param x constant to multiple
		@param b matrix
		@return updated matrix
		~#
		function : native : Multiple(x : Float, b : Float[,]) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := x * b[i,j];
				};
			};

			return c;
		}

		#~
		Multiplies a constant by a matrix
		@param b matrix
		@param x constant to multiple
		@return updated matrix
		~#
		function : native : Multiple(b : Float[,], x : Float) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := b[i,j] * x;
				};
			};

			return c;
		}
		
		#~
		Multiplies two matrices using the Hadamard rule
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Multiple(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

# "a={$a_rows}x{$a_cols}, b={$b_rows}x{$b_cols}"->PrintLine();

			if(a_cols <> b_rows) {
				return Nil;
			}

			out := Float->New[a_rows, b_cols];
			each(i : a_rows) {
				each(k : b_cols) {
					sum := 0.0;

					each(j : a_cols) {
						left := a[i,j];
						right := b[j,k];
						sum += left * right;
					}
					out[i,k] := sum;
				};
			};

			return out;
		}

		#~
		Transpose a matrix swapping rows and columns.
		@param a matrix
		@return transposed matrix
		~#
		function : native : Transpose(a : Float[,]) ~ Float[,] {
			dim := a->Size();
			rows := dim[0];
			cols := dim[1];

			if(rows <> cols) {
				return Nil;
			};

			b := Float->New[cols, rows];
			for(r := 0; r < rows; r += 1;) {
				for(c := 0; c < cols; c += 1;) {
					b[c,r] := a[r,c];
				};
			};

			return b;
		}

		#~
		Calculates the inverse matrix.
		@param a matrix
		@return inverse matrix
		~#
		function : native : Inverse(a : Float[,]) ~ Float[,] {
			dim := a->Size();
			rows := dim[0];
			cols := dim[1];

			if(rows <> cols) {
				return Nil;
			};

"a={$rows}x{$cols}"->PrintLine();

a->ToString()->PrintLine();
"---"->PrintLine();

			count := 0;
			each(i : rows) {
				each(j : cols) {
					if(count % 2 <> 0) {
						a[i,j] := -a[i,j];
					};
					count += 1;
				};
			};

a->ToString()->PrintLine();
"---"->PrintLine();

			a := Transpose(a);
a->ToString()->PrintLine();
"---"->PrintLine();

			return Nil;
		}

		#~
		Calculates the dot product.
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : Dot(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];


# "a=*{$a_cols}x{$a_rows}, b=*{$b_rows}x{$b_cols}"->PrintLine();

			if(a_cols <> b_rows) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(a_col := 0; a_col < a_rows; a_col += 1;) {
				for(right_col := 0; right_col < b_cols; right_col += 1;) {
					cx := 0.0;
					for(x_col := 0; x_col < b_rows; x_col += 1;) {
						cx += a[a_col, x_col] * b[x_col, right_col];
					};
					c[a_col, right_col] := cx;
				};
			};

			return c;
		}

		#~
		Sigmoid 'S' function
		@param x input value
		@return Sigmoid value
		~#
		function : native : Sigmoid(x : Float) ~ Float {
			return 1.0 / (1.0 + Float->Pow(Float->E(), -1.0 * x));
		}

		#~
		Applies the Sigmoid function to all elements
		@param b matrix
		@return updated matrix
		~#
		function : native : Sigmoid(b : Float[,]) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c := Float->New[b_rows, b_cols];
			for(i := 0; i < b_rows; i += 1;) {
				for(j := 0; j < b_cols; j += 1;) {
					c[i,j] := Sigmoid(b[i,j]);
				};
			};

			return c;
		}
		
		#~
		Calculates the Dot Product applying while applying the Sigmoid function to all elements
		@param a matrix
		@param b matrix
		@return updated matrix
		~#
		function : native : DotSigmoid(a : Float[,], b : Float[,]) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];
			
			if(a_cols <> b_rows & a_rows <> b_cols) {
				return Nil;
			};

			c := Float->New[a_rows, b_cols];
			for(a_col := 0; a_col < a_rows; a_col += 1;) {
				for(right_col := 0; right_col < b_cols; right_col += 1;) {
					cx := 0.0;
					for(x_col := 0; x_col < b_rows; x_col += 1;) {
						cx += a[a_col, x_col] * b[x_col, right_col];
					};
					c[a_col, right_col] := Sigmoid(cx);
				};
			};

			return c;
		}

		#~
		Generates a random 2D array of values from 0.0 to 1.0
		@param rows rows
		@param cols columns
		@return updated matrix
		~#
		function : Random(rows : Int, cols : Int) ~ Float[,] {
			m := Float->New[rows, cols];

			for(i := 0; i < rows; i += 1;) {
				for(j := 0; j < cols; j += 1;) {
					m[i,j] := Float->Random();
				};
			};
			
			return m;
		}
		
		#~
		Generates a random normal distribution of values
		@param mean center of values
		@param variance variance in values
		@param rows rows
		@param cols columns
		@return updated matrix
		~#
		function : RandomNormal(mean : Float, variance : Float, rows : Int, cols : Int) ~ Float[,] {
			m := Float->New[rows, cols];

			for(i := 0; i < rows; i += 1;) {
				for(j := 0; j < cols; j += 1;) {
					m[i,j] := RandomNormal(mean, variance);
				};
			};

			return m;
		}

		#~
		Generates a random normal value
		@param mean center of values
		@param variance variance in values
		@return updated matrix
		~#
		function : RandomNormal(mean : Float, variance : Float) ~ Float {
			return  (-2.0 * variance * Float->Random()->Log())->Sqrt() * (2.0 * Float->Pi() * Float->Random())->Cos() + mean;
		}

		#~
		Splits a matrix
		@param b matrix
		@param offset offset index
		@param count number of rows to split
		@param is_row true for row split, false for column
		@return copied matrix
		~#
		function : native : Split(b : Float[,], offset : Int, count : Int, is_row : Bool) ~ Float[,] {
			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];
			
			c : Float[,];
			if(is_row) {
				if(count + offset <= 0 | count + offset > b_rows) {
					return Nil;
				};

				c := Float->New[count, b_cols];
				count := count + offset;
				for(row := offset; row < count; row += 1;) {
					for(col := 0; col < b_cols; col += 1;) {
						c[row - offset, col] := b[row, col];
					};
				};
			}
			else {
				if(count + offset <= 0 | count + offset > b_cols) {
					return Nil;
				};

				c := Float->New[b_rows, count];
				count := count + offset;
				for(row := 0; row < b_rows; row += 1;) {
					for(col := offset; col < count; col += 1;) {
						c[row, col - offset] := b[row, col];
					};
				};
			};

			return c;
		}
		
		#~
		Concatenates two matrix
		@param a left matrix
		@param b right matrix
		@param is_row true concatenate by rows, false for columns
		@return concatenated matrix
		~#
		function : Concatenate(a : Float[,], b : Float[,], is_row : Bool) ~ Float[,] {
			a_dims := a->Size();
			a_rows := a_dims[0];
			a_cols := a_dims[1];

			b_dims := b->Size();
			b_rows := b_dims[0];
			b_cols := b_dims[1];

			c : Float[,];
			if(is_row) {
				if(a_cols <> b_cols) {
					return Nil;
				};

				c_rows := a_rows + b_rows;
				c := Float->New[c_rows, a_cols];

				for(row := 0; row < a_rows; row += 1;) {
					for(col := 0; col < a_cols; col += 1;) {
						c[row, col] := a[row, col];
					};
				};

				for(row := 0; row < b_rows; row += 1;) {
					for(col := 0; col < b_cols; col += 1;) {
						c[row + a_rows, col] := b[row, col];
					};
				};
			}
			else {
				if(a_rows <> b_rows) {
					return Nil;
				};

				c_cols := a_cols + b_cols;
				c := Float->New[a_rows, c_cols];

				for(row := 0; row < a_rows; row += 1;) {
					for(col := 0; col < a_cols; col += 1;) {
						c[row, col] := a[row, col];
					};
				};

				for(row := 0; row < b_rows; row += 1;) {
					for(col := 0; col < b_cols; col += 1;) {
						c[row, col + a_cols] := b[row, col];
					};
				};
			};

			return c;
		}
	}

	#~
	Boolean matrix reference
	~#
	class BoolMatrixRef {
		@value : Bool[,];

		#~
		Default constructor
		~#
		New() {
			Parent();
		}

		#~
		Copy constructor
		@param value boolean value
		~#
		New(value : Bool[,]) {
			Parent();
			@value := value;
		}

		#~
		Returns the number of rows in the matrix
		@return number of rows in the matrix
		~#
		method : public : Rows() ~ Int {
			return @value->Rows();
		}

		#~
		Returns the number of columns in the matrix
		@return number of columns in the matrix
		~#
		method : public : Columns() ~ Int {
			return @value->Columns();
		}

		#~
		Get boolean value
		@return boolean value
		~#
		method : public : Get() ~ Bool[,] {
			return @value;
		}

		#~
		Formats the matrix into a string
		@return string value
		~#
		method : public : ToString() ~ String {
			return @value->ToString();
		}
		
		#~
		Builds random bootstrapped dataset
		@return bootstrapped dataset
		~#
		method : native : public : Bootstrap() ~ BoolMatrixRef {		
			dims := @value->Size();
			rows := dims[0];
			cols := dims[1];
			
			random_rows := Int->New[rows];
			each(i : rows) {
				random_rows[i] := (rows - 1)->Random();
			};
			
			bag_matrix := Bool->New[rows, cols];
			each(i : rows) {
				random_row := random_rows[i];
				each(j : cols) {
					bag_matrix[i, j] := @value[random_row, j];
				};
			};

			return BoolMatrixRef->New(bag_matrix);
		}

		#~
		Splits a 2D boolean matrix
		@param training_offset index of first column with target data. The first element is the train dateset, while the second is the test data
		@return split matrix of training and data sets
		~#
		method : public : Split(training_offset : Float) ~ BoolMatrixRef[] {
			dims := @value->Size();
			rows := dims[0];
			cols := dims[1];

			training_rows := (rows->As(Float) * training_offset)->As(Int);
			training_matrix := Bool->New[training_rows, cols];
			dims := training_matrix->Size();
			train_rows := dims[0];

			each(i : train_rows) {
				each(j : cols) {
					training_matrix[i, j] := @value[i, j];
				};
			};

			data_rows := rows - training_rows;
			data_matrix := Bool->New[data_rows, cols];
			dims := data_matrix->Size();
			data_rows := dims[0];

			each(i : data_rows) {
				each(j : cols) {
					data_matrix[i, j] := @value[training_rows, j];
				};
				training_rows += 1;
			};

			outputs := BoolMatrixRef->New[2];
			outputs[0] := BoolMatrixRef->New(training_matrix);
			outputs[1] := BoolMatrixRef->New(data_matrix);

			return outputs;
		}
	}

	#~
	Float matrix reference with convenience methods
	~#
	class FloatMatrixRef {
		@value : Float[,];

		#~
		Default constructor
		~#
		New() {
			Parent();
		}

		#~
		Copy constructor
		@param value boolean value
		~#
		New(value : Float[,]) {
			Parent();
			@value := value;
		}

		#~
		Get float value
		@return float value
		~#
		method : public : Get() ~ Float[,] {
			return @value;
		}

		method : GetBoolen() ~ Bool {
			dim := @value->Size();
			if(dim->Size() = 2 & dim[0] = 1 & dim[1] = 1) {
				return @value[0,0] = 1.0;
			}
			
			return false;
		}
		
		#~
		Set boolean value
		@param value boolean value		
		~#
		method : public : Set(value : Float[,]) ~ Nil {
			@value := value;
		}

		#~
		Formats the matrix into a string
		@return string value
		~#
		method : public : ToString() ~ String {
			return @value->ToString();
		}

		#~
		Parsers a boolean value
		@return boolean value
		~#
		method : public : ToBool() ~ Bool {
			return ToInt() <> 0;
		}

		#~
		Parsers an integer value
		@return integer value
		~#
		method : public : ToInt() ~ Int {
			return Float->Round(ToFloat());
		}

		#~
		Parsers a decimal value
		@return decimal value
		~#
		method : public : ToFloat() ~ Float {
			dim := @value->Size();
			if(dim->Size() = 2 & dim[0] = 1 & dim[1] = 1) {
				return @value[0,0];
			};

			return 0.0;
		}
	}

	#~
	Simple neural network. Input values should be scaled to between 0.0 to 1.0. 
	The tuned network should return outputs between 0.0 to 1.0.

	```
network : NeuralNetwork;
filename := "data/model.dat";
inputs_targets := MatrixReader->LoadSplitMatrices(args[0], 1, 0.8); # 20% test data

# load model
if(args->Size() = 2) {
   network := NeuralNetwork->Load(filename);
   "Loaded model..."->PrintLine();

   "Testing model..."->PrintLine();
   tests := inputs_targets[2];
   answers := inputs_targets[3];

   failures := 0;
   each(i : answers) {
      answer := answers->Get(i)->ToBool();
      predict := network->Query(FloatMatrixRef->New(tests->Get(i)->Get()));
      if(predict <> answer) {
         failures += 1;
      };
   };

   correct := 100.0 * (1.0 - failures->As(Float) / tests->Size()->As(Float));
   System.IO.Standard->Print("Tests: ")->Print(tests->Size())->Print(", correct: ")->SetFloatPrecision(5)->Print(correct)->PrintLine("%");
}
# train and store model
else if(args->Size() = 1) {
   "Training model..."->PrintLine();
   network := NeuralNetwork->Train(2, inputs_targets[0], 8, 1, inputs_targets[1], 0.01725, 256);
   if(inputs_targets <> Nil) {
      network->Store(filename);
      "Stored model..."->PrintLine();
   };
}
	```
	~#
	class NeuralNetwork {
		@input_nodes : Float;
		@hidden_nodes : Float;
		@output_nodes : Float;
		@learning_rate : Float;
		@weight_inputs_hidden : Float[,];
		@weight_outputs_hidden : Float[,];
		@threshold : Float;
		@attempts : Int;

		#~
		Trains the network
		@param input_nodes number of input nodes
		@param inputs : training inputs
		@param hidden_factor size of hidden layer, factor of input (i.e. input_nodes * hidden_factor)
		@param output_nodes training outputs
		@param targets training targets
		@param learning_rate learning rate
		@param iterations number of training iterations
		~#
		function : Train(input_nodes : Int, inputs : Vector<FloatMatrixRef>, hidden_factor : Int, output_nodes : Int, targets : Vector<FloatMatrixRef>, learning_rate : Float, iterations : Int) ~ NeuralNetwork {
			network := NeuralNetwork->New(input_nodes, hidden_factor * input_nodes, output_nodes, learning_rate);
			network->Train(inputs, targets, learning_rate, iterations);
			return network;		
		}
		
		New : private (input_nodes : Float, hidden_nodes : Float, output_nodes : Float, learning_rate : Float) {
			@input_nodes := input_nodes;
			@hidden_nodes := hidden_nodes;
			@output_nodes := output_nodes;
			@learning_rate := learning_rate;
			
			@threshold := 0.80;
			@attempts := 10;

			@weight_inputs_hidden := Matrix2D->RandomNormal(0.0, Float->Pow(@input_nodes, -1.0), @hidden_nodes, @input_nodes);
			@weight_outputs_hidden := Matrix2D->RandomNormal(0.0, Float->Pow(@input_nodes, -1.0), @output_nodes, @hidden_nodes);
		}

		New : private (weight_inputs_hidden : Float[,], weight_outputs_hidden : Float[,]) {
			@weight_inputs_hidden := weight_inputs_hidden;
			@weight_outputs_hidden := weight_outputs_hidden;

			@threshold := 0.80;
			@attempts := 10;
		}

		#~
		Sets the activation threshold. Activation if output > threshold or < 1.0 - threshold
		@param threshold activation threshold default is 0.85
		@param attempts number of activation attempts
		~#
		method : public : SetActivation(threshold : Float, attempts : Int) ~ Nil {
			@threshold := threshold;
			@attempts := attempts;
		}

		#~
		Query the network
		@param inputs query inputs
		@return true if activated, false otherwise
		~#
		method : public : Query(inputs : FloatMatrixRef) ~ Bool {
			# try and fail is unsuccessful
			each(i : @attempts) {
				result := Confidence(inputs);
				if(result > @threshold) {
					return true;
				};		
			};

			return false;
		}

		#~
		Query the network's confidence for the give inputs
		@param inputs query inputs
		@return confidence percentage
		~#
		method : public : Confidence(inputs : FloatMatrixRef) ~ Float {
			outputs := Query(inputs->Get());
			if(outputs <> Nil) {
				dims := outputs->Size();
				if(dims->Size() = 2 & dims[0] = 1 & dims[1] = 1) {
					return outputs[0,0];
				};
			};

			return 0.0
		}

		#~
		Loads network inputs and outputs
		@param filename file to store to
		@return true if successful, false otherwise
		~#
		function : Load(filename : String) ~ NeuralNetwork {
			data := System.IO.Filesystem.FileReader->ReadBinaryFile(filename);
			if(data <> Nil) {
				# read inputs weight
				deserializer := System.IO.Deserializer->New(data);
				height := deserializer->ReadInt();
				width := deserializer->ReadInt();

				weight_inputs_hidden := Float->New[height, width];
				each(i : height) {
					each(j : width) {
						weight_inputs_hidden[i,j] := deserializer->ReadFloat();
					};
				};
				height := deserializer->ReadInt();
				width := deserializer->ReadInt();

				weight_outputs_hidden := Float->New[height, width];
				each(i : height) {
					each(j : width) {
						weight_outputs_hidden[i,j] := deserializer->ReadFloat();
					};
				};

				return NeuralNetwork->New(weight_inputs_hidden, weight_outputs_hidden);
			}

			return Nil;
		}

		#~
		Saves final network inputs and outputs
		@param filename file to store to
		@return true if successful, false otherwise
		~#
		method : public : Store(filename : String) ~ Bool {
			input_dims := @weight_inputs_hidden->Size();
			if((input_dims->Size() = 2 & input_dims[0] > 0 & input_dims[1] > 0) = false) {
				return false;
			}

			# write inputs weight
			height := input_dims[0];
			width := input_dims[1];

			serializer := System.IO.Serializer->New();

			serializer->Write(height);
			serializer->Write(width);

			each(i : height) {
				each(j : width) {
					serializer->Write(@weight_inputs_hidden[i,j])
				};
			};
			# write outputs weight
			output_dims := @weight_outputs_hidden->Size();
			if((output_dims->Size() = 2 & output_dims[0] > 0 & output_dims[1] > 0) = false) {
				return false;
			}

			# write outputs weight
			height := output_dims[0];
			width := output_dims[1];

			serializer->Write(height);
			serializer->Write(width);

			each(i : height) {
				each(j : width) {
					serializer->Write(@weight_outputs_hidden[i,j])
				};
			};
			
			return System.IO.Filesystem.FileWriter->WriteFile(filename, serializer->Serialize());
		}

		method : Query(inputs : Float[,]) ~ Float[,] {
			# calculate signals into hidden layer
			hidden_outputs := Matrix2D->DotSigmoid(@weight_inputs_hidden, inputs);
			# calculate the signals emerging from final output layer
			return Matrix2D->DotSigmoid(@weight_outputs_hidden, hidden_outputs);
		}

		method : Train(inputs : Vector<FloatMatrixRef>, targets : Vector<FloatMatrixRef>, rate : Float, iterations : Int) ~ Nil {
			if(inputs->Size() = targets->Size()) {
				each(i : iterations) {
					each(j : inputs) {
						input := inputs->Get(j)->Get();
						target := targets->Get(j)->Get();

						# calculate signals into hidden layer
						hidden_outputs := Matrix2D->DotSigmoid(@weight_inputs_hidden, input);
						# calculate signals into final output layer
						final_outputs  := Matrix2D->DotSigmoid(@weight_outputs_hidden, hidden_outputs);
						# output layer error is the (target - actual)
						output_errors := Matrix2D->Subtract(target, final_outputs);
						# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
						hidden_errors := Matrix2D->Dot(Matrix2D->Transpose(@weight_outputs_hidden), output_errors);
						# update the weights for the links between the input and hidden layers
						@weight_inputs_hidden := Matrix2D->Add(@weight_inputs_hidden, Adjust(rate, hidden_errors, hidden_outputs, input));
						# update the weights for the links between the hidden and output layers
						@weight_outputs_hidden := Matrix2D->Add(@weight_outputs_hidden, Adjust(rate, output_errors, final_outputs, hidden_outputs));
					};
				};
			};
		}

		method : Adjust(rate : Float, errors : Float[,], outputs : Float[,], inputs : Float[,]) ~ Float[,] {
			return Matrix2D->Multiple(rate, Matrix2D->Dot(Matrix2D->Multiple(errors, Matrix2D->Multiple(outputs, Matrix2D->Subtract(1.0, outputs))), Matrix2D->Transpose(inputs)));
		}
	}

	#~
	Utilities for reading data from CSV data sources 
	~#
	class MatrixReader {
		#~
		Load input and output data into matrices
		@param filename file to process
		@param target_offset index of first column with target data
		@return input and output matrix data
		~#
		function : public : LoadMatrices(filename : String, target_offset : Int) ~ Vector[]<FloatMatrixRef> {
			table := CsvTable->New(System.IO.Filesystem.FileReader->ReadFile(filename));
			if(table->IsParsed()) {
				inputs := Vector->New()<FloatMatrixRef>;
				targets := Vector->New()<FloatMatrixRef>;

				each(row := table) {
					input_length := row->Size() - target_offset;
					target_length := row->Size() - input_length;
					if(input_length < 0) {
						return Nil;
					};

					input_array := Float->New[input_length, 1];
					target_array := Float->New[target_length, 1];

					each(i : input_length) {
						input_array[i, 0] := row->Get(i)->ToFloat();
					};

					target_index := 0;
					for(i := input_length; i < row->Size(); i += 1;) {
						target_array[target_index, 0] := row->Get(i)->ToFloat();
					};

					inputs->AddBack(FloatMatrixRef->New(input_array));
					targets->AddBack(FloatMatrixRef->New(target_array));
				};

				input_targets := Vector->New[2]<FloatMatrixRef>;
				input_targets[0] := inputs;
				input_targets[1] := targets;

				return input_targets;
			};

			return Nil;
		}

		#~
		Load input and output data into split matrices for training and testing
		@param filename file to process
		@param target_offset index of first column with output data
		@param training_perc percentage of data used for training
		@return split matrices for training and testing, first two matrices are training, latter two test data
		~#
		function : public : LoadSplitMatrices(filename : String, target_offset : Int, training_perc : Float) ~ Vector[]<FloatMatrixRef> {
			table := CsvTable->New(System.IO.Filesystem.FileReader->ReadFile(filename));
			if(table->IsParsed()) {
				train_count := (table->Size()->As(Float) * training_perc)->As(Int);
				if(train_count < 1) {
					return Nil;
				};

				training_inputs := Vector->New()<FloatMatrixRef>;
				training_targets := Vector->New()<FloatMatrixRef>;

				test_inputs := Vector->New()<FloatMatrixRef>;
				test_targets := Vector->New()<FloatMatrixRef>;

				count := 0;
				each(row := table) {
					input_length := row->Size() - target_offset;
					target_length := row->Size() - input_length;
					if(input_length < 0) {
						return Nil;
					};

					input_array := Float->New[input_length, 1];
					target_array := Float->New[target_length, 1];

					each(i : input_length) {
						input_array[i, 0] := row->Get(i)->ToFloat();
					};

					target_count := 0;
					for(i := input_length; i < row->Size(); i += 1;) {
						target_array[target_count, 0] := row->Get(i)->ToFloat();
					};

					if(count < train_count) {
						training_inputs->AddBack(FloatMatrixRef->New(input_array));
						training_targets->AddBack(FloatMatrixRef->New(target_array));
					}
					else {
						test_inputs->AddBack(FloatMatrixRef->New(input_array));
						test_targets->AddBack(FloatMatrixRef->New(target_array));
					};

					count += 1;
				};

				input_training_targets := Vector->New[4]<FloatMatrixRef>;
				input_training_targets[0] := training_inputs;
				input_training_targets[1] := training_targets;
				input_training_targets[2] := test_inputs;
				input_training_targets[3] := test_targets;

				return input_training_targets;
			};

			return Nil;
		}
	}

	#~
	Random forest algorithm
```
forest := RandomForest->New(8);
forest->Train(0.3, data);
result := forest->Query(data);

possible := Bool->Rows(result)->As(Float);
matched := DecisionTree->Matches(result->Columns() - 1, result)->As(Float);
matched_perc := (matched / possible * 100.0)->As(Int);
"matched {$matched_perc}%"->PrintLine();
```
	~#
	class RandomForest {
		@decisions : Vector<Split>;
		@num_trees : Int;

		#~
		Constructor
		@param num_trees number of trees to generate
		~#
		New(num_trees : Int) {
			@num_trees := num_trees;
		}

		New : private (num_trees : Int, decisions : Vector<Split>) {
			@decisions := decisions;
			@num_trees := num_trees;
		}

		#~
		Calculates a list of decision splits
		@param split_perc percentage of data to use for training
		@param in training matrix
		~#
		method : public : native : Train(split_perc : Float, in : BoolMatrixRef) ~ Nil {
			Train(split_perc, in->Get());
		}

		#~
		Calculates a list of decision splits
		@param split_perc percentage of data to use for training
		@param in training matrix
		~#
		method : public : native : Train(split_perc : Float, in : Bool[,]) ~ Nil {
			data := BoolMatrixRef->New(in);
			best_match := 0.0;

			each(i : @num_trees) {				
				split_data := data->Split(split_perc);
				training_data := split_data[0];
				test_data := split_data[1];

				decisions := DecisionTree->Train(training_data->Get());
				result := DecisionTree->Query(decisions, test_data->Get());

				possible := result->Rows()->As(Float);
				acheived := DecisionTree->Matches(2, result)->As(Float);

				outcome := acheived / possible;
				if(outcome > best_match) {
					best_match := outcome;
					@decisions := decisions; 
				};
				data := data->Bootstrap();
			};
		}

		#~
		Splits a matrix based on a list of decisions
		@param in matrix to be split
		@return split matrix
		~#
		method : public : native : Query(in : BoolMatrixRef) ~ Bool[,] {
			return Query(in->Get());
		}

		#~
		Splits a matrix based on a list of decisions
		@param in matrix to be split
		@return split matrix
		~#
		method : public : native : Query(in : Bool[,]) ~ Bool[,] {
			if(@decisions <> Nil) {
				return DecisionTree->Query(@decisions, in);
			};

			return Nil;
		}

		#~
		Loads a saved random forest
		@param filename file to store to
		@return random forest
		~#
		function : Load(filename : String) ~ RandomForest {
			data := System.IO.Filesystem.FileReader->ReadBinaryFile(filename);
			if(data <> Nil) {
				deserializer := System.IO.Deserializer->New(data);
				
				num_trees := deserializer->ReadInt();
				
				decisions := Vector->New()<Split>;
				num_decisions := deserializer->ReadInt();
				each(i : num_decisions) {
					decisions->AddBack(deserializer->ReadObject()->As(Split));
				};

				return RandomForest->New(num_trees, decisions);
			}

			return Nil;
		}

		#~
		Saves a random forest
		@param filename file to save to
		@return true if successful, false otherwise
		~#
		method : public : Store(filename : String) ~ Bool {
			if(@decisions <> Nil) {
				serializer := System.IO.Serializer->New();

				serializer->Write(@num_trees);
				
				serializer->Write(@decisions->Size());
				each(decision : @decisions) {
					serializer->Write(decision);
				};

				return System.IO.Filesystem.FileWriter->WriteFile(filename, serializer->Serialize());
			};

			return false;
		}
	}

	#~
	Binary decision tree algorithm
	
```
data := BoolMatrixRef->New([
  [true, false, true]
  [true, false, true]
  [true, true, true]
  [true, true, true]
  [true, true, true]
  [true, true, true]
  [true, true, true]
  [true, true, true]
  [false, true, true]
  [false, true, true]
  [true, true, false]
  [true, true, false]
  [false, true, false]
  [false, true, false]
  [false, true, false]
  [false, true, false]
  [false, false, false]
  [false, false, false]
  [false, false, false]
  [false, false, false]
]);
split_data := data->Split(0.3);
training_data := split_data[0];
test_data := split_data[1];

decisions := DecisionTree->Split(training_data->Get());
result := DecisionTree->Decide(decisions, test_data->Get());

possible := result->Rows()->As(Float);
acheived := DecisionTree->Matches(2, result)->As(Float);
(acheived / possible)->PrintLine();
```
	~#
	class DecisionTree {
		#~
		Splits a matrix based on a list of decisions
		@param decisions list of decisions
		@param in matrix to be split
		@return split matrix
		~#
		function : native : Query(decisions : Vector<Split>, in : Bool[,]) ~ Bool[,] {
			each(decision := decisions) {
				out := Split(decision->GetIndex(), in);
				if(out = Nil) {
					return in;
				};
				in := out;
			}

			return in;
		}

		#~
		Calculates a list of decision splits
		@param in training matrix
		@return list of decision tree splits
		~#
		function : native : Train(in : Bool[,]) ~ Vector<Split> {					
			decisions := Vector->New()<Split>;
			dims := in->Size();
			
			cols := dims[1] - 1;
			if(cols > 0) {
				done := false;
				while(<>done) {
					best_wgi := 1000000.0;
					best_index := -1;

					each(i : cols) {
						wgi := WeightedGini(i, cols, in);
						if(wgi <> 0.0 & wgi < best_wgi) {
							best_wgi := wgi;
							best_index := i;
						}
					}

					if(best_index > - 1) {
						in := Split(best_index, in);
						decisions->AddBack(Split->New(best_index, best_wgi));

					}
					else {
						done := true;
					};
				};
			};

			return decisions;
		}

		#~
		Splits a matrix along the given column
		@param index index used to split
		@param in matrix to split
		@return split matrix
		~#
		function : native : Split(index : Int, in : Bool[,]) ~ Bool[,] {
			split_len := Matches(index, in);
			if(split_len > 0) {
				dims := in->Size();
				rows := dims[0];
				cols := dims[1];

				out := Bool->New[split_len, cols];
				count := 0;
				if(index > -1 & index < rows) {
					each(row : rows) {
						if(in[row, index]) {
							each(k : cols) {
								out[count, k] := in[row, k];
							};
							count += 1;
						};
					};
				};

				return out;
			};

			return Nil;
		}

		#~
		Calculates the Gini index
		@param acheived number acheived
		@param goal achievement target
		@return Gini index
		~#
		function : native : Gini(acheived : Float, goal : Float) ~ Float {
			reached := acheived / goal;
			not_reached := (goal - acheived) / goal;
			return 1 - (reached*reached + not_reached*not_reached);
		}

		function : native : WeightedGini(test : Int, target : Int, in : Bool[,]) ~ Float {
			wgi := 0.0;

			dims := in->Size();
			size := dims[0]->As(Float);

			# left
			goal := Matches(test, in);
			if(goal > 0.0) {
				wgi += goal / size * Gini(Acheived(test, target, in), goal);
			}
			else {
				return 0.0;
			};

			
			# right
			goal := Mismatches(test, in);
			if(goal > 0.0) {
				wgi += goal / size * Gini(Unacheived(test, target, in), goal);
			}
			else {
				return 0.0;
			};

			return wgi;
		}

		#~
		Count matches in a column
		@param index column index
		@param matrix matrix to inspect
		@return number of matches
		~#
		function : Matches(index : Int, matrix : Bool[,]) ~ Int {
			dims := matrix->Size();
			rows := dims[0];

			count := 0;
			if(index > -1 & index < rows) {
				each(row : rows) {
					if(matrix[row, index]) {
						count += 1;
					};
				};
			};

			return count;
		}

		#~
		Count mismatches in a column
		@param index column index
		@param matrix matrix to inspect
		@return number of mismatches
		~#
		function : Mismatches(index : Int, matrix : Bool[,]) ~ Int {
			dims := matrix->Size();
			rows := dims[0];

			count := 0;
			if(index > -1 & index < rows) {
				each(row : rows) {
					if(matrix[row, index] = false) {
						count += 1;
					};
				};
			};

			return count;
		}

		function : native : Acheived(test_index : Int, goal_index : Int, matrix : Bool[,]) ~ Float {
			dims := matrix->Size();
			rows := dims[0];

			count := 0;
			if(test_index > -1 & test_index < rows & goal_index > -1 & goal_index < rows) {
				each(row : rows) {
					if(matrix[row, test_index] & matrix[row, goal_index]) {
						count += 1;
					};
				};
			};

			return count;
		}

		function : native : Unacheived(test_index : Int, goal_index : Int, matrix : Bool[,]) ~ Float {
			dims := matrix->Size();
			rows := dims[0];

			count := 0;
			if(test_index > -1 & test_index < rows & goal_index > -1 & goal_index < rows) {
				each(row : rows) {
					if(matrix[row, test_index] = false & matrix[row, goal_index]) {
						count += 1;
					};
				};
			};

			return count;
		}

		#~
		Loads a boolean input matrix from a CSV file of 1s and 0s
		@param filename CSV file to load
		@return boolean matrix
		~#
		function : LoadCsv(filename : String) ~ Bool[,] {
			table := CsvTable->New(System.IO.Filesystem.FileReader->ReadFile(filename));
			if(table->IsParsed()) {
				row_size := table->Size();
				if(row_size->Size() > 1) {
					column := table->Get(1);
					column_size := column->Size();

					matrix := Bool->New[row_size - 1, column_size];
					for(i := 1; i < row_size; i += 1;) {
						each(j : column_size) {
							value := table->Get(i)->Get(j)->ToLower();
							if(value->Equals(["0", "0.0", "false", "o", "f"])) {
								matrix[i - 1, j] := false;
							}
							else {
								matrix[i - 1, j] := true;	
							};
						};
					};

					return matrix;
				};
			};

			return Nil;
		}
	}

	class Split {
		@index : Int;
		@weighted_gini : Float;
		
		New(index : Int, weighted_gini : Float) {
			@index := index;
			@weighted_gini := weighted_gini;
		}

		method : public : GetWeightedGini() ~ Float {
			return @weighted_gini;
		}

		method : public : GetIndex() ~ Int {
			return @index;
		}

		method : public : ToString() ~ String {
			return "index={$@index},weighted_gini={$@weighted_gini}";
		}
	}
	
	#~
	Naive Bayes entry of term with counts
	~#
	class BayesEntry {
		@name : String;
		@count : Int;
		@bayes : Float;

		#~
		Constructor
		@param name entry name
		@param count count of occurrences 
		~#
		New(name : String, count : Int) {
			@name := name;
			@count := count;
		}

		#~
		Get the entry name
		@return entry name
		~#
		method : public : GetName() ~ String {
			return @name;
		}

		#~
		Get the entry count
		@return entry count
		~#
		method : public : GetCount() ~ Int {
			return @count;
		}

		#~
		Set the entry count
		@param count entry count
		~#
		method : public : SetCount(count : Int) ~ Nil {
			@count := count;
		}
	}

	#~
	Naive Bayes group of entries
	~#
	class BayesGroup {
		@entries : Map<String, BayesEntry>;
		@prob : Float;

		#~
		Sets the initial probability
		@param prob initial probability
		~#
		New(prob : Float) {
			@prob := prob;
			@entries := Map->New()<String, BayesEntry>;
		}

		#~
		Adds an entry
		@param entry entry to add
		@return true if added, false otherwise
		~#
		method : public : AddEntry(entry : BayesEntry) ~ Bool {
			if(@entries->Has(entry->GetName())) {
				return false;
			};

			@entries->Insert(entry->GetName(), entry);
			return true;
		}

		#~
		Gets entries
		@return entries
		~#
		method : public : GetEntries() ~ Map<String, BayesEntry> {
			return @entries;
		}

		#~
		Gets the initial probability
		@return probability
		~#
		method : public : GetProbability() ~ Float {
			return @prob;
		}
	}

	#~
	Naive Bayes ML algorithm

```
normal := BayesGroup->New(8.0 / 12.0);
normal->AddEntry(BayesEntry->New("dear", 8));		
normal->AddEntry(BayesEntry->New("friend", 5));
normal->AddEntry(BayesEntry->New("lunch", 3));
normal->AddEntry(BayesEntry->New("money", 1));

spam := BayesGroup->New(4.0 / 12.0);
spam->AddEntry(BayesEntry->New("dear", 2));
spam->AddEntry(BayesEntry->New("friend", 1));
spam->AddEntry(BayesEntry->New("lunch", 0));
spam->AddEntry(BayesEntry->New("money", 4));

bayes := NaiveBayes->New(normal, spam);
finding := bayes->Query(["lunch", "money", "money", "money", "money"]);

if(finding = 0) {
  "normal"->PrintLine();
}
else if(finding = 1) {
  "spam"->PrintLine();	
}
else {
  "invalid"->PrintLine();	
}	
	```
	~#
	class NaiveBayes {
		@left : BayesGroup;
		@right : BayesGroup;

		#~
		Constructor
		@param left left group
		@param right right group
		~#
		New(left : BayesGroup, right : BayesGroup) {
			@left := left;
			@right := right;
		}

		#~
		Query to Naive Bayes
		@param terms list of term to calculate
		@return 0 if left, 1 if right, -1 otherwise
		~#
		method : public : Query(terms : String[]) ~ Int {
			left := Query(terms, @left);
			right := Query(terms, @right);

# "left={$left}, right={$right}"->PrintLine();

			if(left = 0.0 | right = 0.0) {
				return -1;
			};

			if(left > right) {
				return 0;
			};

			return 1;
		}

		method : Query(terms : String[], group : BayesGroup) ~ Float {
			entries := group->GetEntries();
			prob := group->GetProbability();

			sum := 0.0;
			values := entries->GetValues()<BayesEntry>;
			each(value := values) {
				sum += value->GetCount();
			};

			index := 0;
			results := Map->New()<String, FloatRef>;
			each(value := values) {
				if(value->GetCount() = 0) {
					value->SetCount(1);
					sum += values->Size();
				};
				count := value->GetCount()
				name := value->GetName();
				result := count / sum;
				results->Insert(name, result);

# "{$name}: ({$count}/{$sum})={$result} * {$prob}"->PrintLine();
				
				index += 1;
			};

			result :=  0.0;
			each(term := terms) {
				find := results->Find(term);
				if(find <> Nil) {
					if(result = 0.0) {
						result := find;
					}
					else {
						result *= find;
					};
				};
			};

			return prob * result;
		}		
	}

	#~
	K-Nearest Neighbors algorithm

```
matrix := [
  [51.0,  167.0]
  [58.0, 169.0]
  [62.0, 182.0]
  [69.0, 176.0]
  [64.0, 173.0]
  [65.0, 172.0]
  [56.0, 174.0]
  [57.0, 173.0]
  [55.0, 170.0]];

labels := [
  "underweight", 
  "normal", 
  "normal", 
  "normal", 
  "normal", 
  "underweight", 
  "normal", 
  "normal", 
  "normal"];

knn := KNearestNeighbors->New(matrix, labels);
nearest := knn->Query(3, [57.0, 170.0]);
each(neighbor := nearest) {
  neighbor->ToString()->PrintLine();
};
```
~#
	class KNearestNeighbors {
		@matrix : Float[,];
		@labels : String[];

		#~
		Constructor.
		@param matrix input matrix
		@param labels categories labels
		~#
		New(matrix : Float[,], labels : String[]) {
			@matrix := matrix;
			@labels := labels;
		}

		#~
		Query the matrix for classification
		@param k number of nearest neighbors 
		@param query input query
		@return k nearest neighbors 
		~#
		method : public : Query(k : Int, query : Float[]) ~ KNeighbor[] {
			return Query(query, k);		
		}

		method : native : Query(query : Float[], k : Int) ~ KNeighbor[] {
			dims := @matrix->Size();
			rows := dims[0];
			cols := dims[1];

			distances := Collection.CompareVector->New()<KNeighbor>;
			row := Float->New[cols];	
			each(i : rows) {
				each(j : cols) {
					row[j] := @matrix[i,j];
				};
				dist := EuclideanDistance(row, query);
				distances->AddBack(KNeighbor->New(dist, i, @labels[i]));
			};

			distances->Sort();			
			results := KNeighbor->New[k];
			i := distances->Size() - 1;
			while(k > 0) {
				results[--k] := distances->Get(i--);
			};

			return results;
		}

		method : native : EuclideanDistance(a : Float[], b : Float[]) ~ Float {
			if(a->Size() <> b->Size()) {
				return 0.0;
			};

			total := 0.0;
			
			each(i : a) {
				diff := b[i] - a[i];
				total += diff * diff;
			};

			return total->Sqrt();
		}
	}

	#~
	K-Neighbor result
	~#
	class KNeighbor implements Compare {
		@dist : Float;
		@row : Int;
		@cat : String;

		#~
		Constructor.
		@param dist distance value
		@param row row index
		@param cat category label
		~#
		New(dist : Float, row : Int, cat : String) {
			@dist := dist;
			@row := row;
			@cat := cat;
		}

		#~
		Compares neighbors 
		@param rhs neighbor to compare
		@return 0 if equal, -1 if types differ, 1 if equal
		~#
		method : public : Compare(rhs:System.Compare) ~ Int {
			if(@self->GetClassID() <> rhs->GetClassID()) {
				return -1;
			};
			right : KNeighbor := rhs->As(KNeighbor);
			bar := right->GetDistance();
			# foo := Float->Compare(bar, @dist);

			if(bar < @dist) {
				return -1;
			}
			else if(bar > @dist) {
				return 1;
			};

			return 0;
		}

		#~
		Returns a unique hash ID for a boolean
		@return hash ID
		~#
		method : public : HashID() ~ Int {
			return @dist * 4000000;
		}

		#~
		Get the distance
		@return distance
		~#
		method : public : GetDistance() ~ Float {
			return @dist;
		}

		#~
		Get the row index
		@return row index
		~#
		method : public : GetRow() ~ Int {
			return @row;
		}

		#~
		Get the category label
		@return category label
		~#
		method : public : GetCategory() ~ String {
			return @cat;
		}

		#~
		Returns a string representation of the neighbor
		@return string representation of the neighbor
		~#
		method : public : ToString() ~ String {
			return "{$@row}: distance={$@dist}, category={$@cat}";
		}
	}

#~
	K-Means clustering algorithm
	
	```
	points := Vector->New()<KMeansPoint>;
	points->AddBack(KMeansPoint->New(9.78, 7.86));
	points->AddBack(KMeansPoint->New(0.81, 0.77));
	points->AddBack(KMeansPoint->New(8.82, 5.48));
	points->AddBack(KMeansPoint->New(1.80, 0.43));
	points->AddBack(KMeansPoint->New(6.16, 6.60));
	points->AddBack(KMeansPoint->New(6.51, 8.72));
	points->AddBack(KMeansPoint->New(1.10, 2.56));
	points->AddBack(KMeansPoint->New(7.89, 4.49));
	points->AddBack(KMeansPoint->New(3.33, 2.89));
	points->AddBack(KMeansPoint->New(0.74, 3.99));
	points->AddBack(KMeansPoint->New(3.15, 1.70));
	points->AddBack(KMeansPoint->New(0.55, 6.27));

	kmeans := KMeans->New(2, 0.0, 10.0, points);
	clusters := kmeans->Cluster();

	each(i : clusters) {
	  cluster := clusters[i];
	  "G{$i}->{$cluster}"->PrintLine();
	};
	```
~#
	class KMeans {
		@points : Vector<KMeansPoint>;
		@k : Int;
		@min : Float;
		@max : Float;
		@debug : Bool;

		#~
		Constructor
		@param k k number of clusters
		@param min minimum starting centroid point
		@param max maximum starting centroid point
		@param points input data points
		~#
		New(k : Int, min : Float, max : Float, points : Vector<KMeansPoint>) {
			@k := k;
			@min := min;
			@max := max;
			@points := points;

			@debug := false;
		}

		#~
		Cluster input points
		@return clustered input points
		~#
		method : public : Cluster() ~ Vector[]<KMeansPoint> {
			centroids := InitializeCentroids();
			k := centroids->Size();

			buckets := Vector->New[k]<KMeansPoint>;
			each(i : k) {
				buckets[i] := Vector->New()<KMeansPoint>;
			};
			results := Distance->New[k];

			last_centroids := KMeansPoint->New[k];
			each(i : last_centroids) {
				last_centroids[i] := KMeansPoint->New();
			};

			converged := false;
			do {
				if(@debug) {
					"\n=>{$centroids}\n------"->PrintLine();
				};

				each(point := @points) {
					each(i : centroids) {
						centroid := centroids->Get(i);
						distance := GetDistance(centroid, point);
						result := Distance->New(point, distance);

						if(@debug) {
							"{$centroid}: {$result}"->PrintLine();
						};
						results[i] := result;
					};

					if(@debug) {
						"------"->PrintLine();
					};

					pt_index := -1;
					min_dist := Float->Inf();
					each(i : results) {
						result := results[i];
						if(result->GetDistance() < min_dist) {
							min_dist := result->GetDistance();
							pt_index := i;
						}
					};

					result := results[pt_index];
					bucket := buckets[pt_index];
					bucket->AddBack(result->GetPoint());
				};

				each(i : k) {
					last_centroids[i] := centroids->Get(i);
				};
				
				centroids->Empty();
				each(i : k) {
					bucket := buckets[i];

					x_update := y_update := 0.0;
					each(item := bucket) {
						x_update += item->GetX();
						y_update += item->GetY();
					};
					x_update /= bucket->Size()->As(Float);
					y_update /= bucket->Size()->As(Float);

					if(@debug) {
						"C{$i}->{$x_update}, {$y_update}"->PrintLine();
					};

					centroids->AddBack(KMeansPoint->New(x_update, y_update));
				};

				converged := HasConverged(centroids, last_centroids);
				if(<>converged) {
					each(i : k) {
						buckets[i]->Empty();
					};
				}
			}
			while(<>converged);

			return buckets;
		}

		method : native : HasConverged(current : Vector<KMeansPoint>, last : KMeansPoint[]) ~ Bool {
			each(i : current) {
				left := current->Get(i);
				right := last[i];

				if(<>left->Equals(right)) {
					return false;
				};
			}

			return true;
		}

		method : native : GetDistance(a : KMeansPoint, b : KMeansPoint) ~ Float {
			x1 := a->GetX(); y1 := a->GetY();
			x2 := b->GetX(); y2 := b->GetY();

			return Float->Sqrt(Float->Pow(x2 - x1, 2.0) + Float->Pow(y2 - y1, 2.0));
		}

		method : InitializeCentroids() ~ Vector<KMeansPoint> {
			centroids := Vector->New()<KMeansPoint>;

			each(i : @k) {
				centroids->AddBack(KMeansPoint->New(Float->Random(@min, @max), Float->Random(@min, @max)));
			};

			return centroids;
		}

		#~
		Load input data point from a CSV file
		@param filename input filename
		@return input data points
		~#
		function : LoadData(filename : String) ~ Vector<KMeansPoint> {
			points := Vector->New()<KMeansPoint>;
			
			table := CsvTable->New(System.IO.Filesystem.FileReader->ReadFile(filename));
			if(table->IsParsed()) {
				each(row := table) {
					if(row->Size() <> 2) {
						return Nil;
					};

					x := row->Get(0)->ToFloat();
					y := row->Get(1)->ToFloat();

					points->AddBack(KMeansPoint->New(x, y));
				};
			};

			return points;
		}
	}

	class : private : Distance implements Stringify {
		@point : KMeansPoint;
		@distance : Float;

		New(point : KMeansPoint, distance : Float) {
			@point := point;
			@distance := distance;
		}

		method : public : GetPoint() ~ KMeansPoint {
			return @point;
		}

		method : public : GetDistance() ~ Float {
			return @distance;
		}

		method : public : ToString() ~ String {
			return "{$@point}: {$@distance}";
		}
	}

	#~
	K-Means point
	~#
	class KMeansPoint implements Stringify {
		@x : Float;
		@y : Float;

		#~
		Default constructor
		~#
		New() {
			@x := 0.0;
			@y := 0.0;
		}

		#~
		Constructor
		@param x x value
		@param y y value
		~#
		New(x : Float, y : Float) {
			@x := x;
			@y := y;
		}

		#~
		Get x value
		@return x value
		~#
		method : public : GetX() ~ Float {
			return @x;
		}

		#~
		Get y value
		@return y value
		~#
		method : public : GetY() ~ Float {
			return @y;
		}

		#~
		Test if two points are equal
		@param point compare point
		@return true if equal, false otherwise
		~#
		method : public : Equals(point : KMeansPoint) ~ Bool {
			return point->GetX() = @x & point->GetY() = @y;
		}

		#~
		String representation of the object
		@return representation of the object
		~#
		method : public : ToString() ~ String {
			return "{{$@x}, {$@y}}";
		}
	}
}
